

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>tptt package &mdash; TPTT 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=01f34227"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="TPTT Documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            TPTT
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">tptt package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-tptt.configuration_tptt">tptt.configuration_tptt module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig"><code class="docutils literal notranslate"><span class="pre">TpttConfig</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig.RECURRENT_MODES"><code class="docutils literal notranslate"><span class="pre">TpttConfig.RECURRENT_MODES</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig.architectures"><code class="docutils literal notranslate"><span class="pre">TpttConfig.architectures</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig.auto_map"><code class="docutils literal notranslate"><span class="pre">TpttConfig.auto_map</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig.model_type"><code class="docutils literal notranslate"><span class="pre">TpttConfig.model_type</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.configuration_tptt.convert_sets_to_lists"><code class="docutils literal notranslate"><span class="pre">convert_sets_to_lists()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.configuration_tptt.extract_template_variables"><code class="docutils literal notranslate"><span class="pre">extract_template_variables()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.configuration_tptt.generate_model_card"><code class="docutils literal notranslate"><span class="pre">generate_model_card()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.configuration_tptt.get_mode_name"><code class="docutils literal notranslate"><span class="pre">get_mode_name()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.configuration_tptt.parse_mode_name"><code class="docutils literal notranslate"><span class="pre">parse_mode_name()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-tptt.modeling_tptt">tptt.modeling_tptt module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.CausalAvgPool1d"><code class="docutils literal notranslate"><span class="pre">CausalAvgPool1d</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.CausalAvgPool1d.forward"><code class="docutils literal notranslate"><span class="pre">CausalAvgPool1d.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.LCache"><code class="docutils literal notranslate"><span class="pre">LCache</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LCache.reset"><code class="docutils literal notranslate"><span class="pre">LCache.reset()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LCache.update"><code class="docutils literal notranslate"><span class="pre">LCache.update()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.LiZAttention"><code class="docutils literal notranslate"><span class="pre">LiZAttention</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LiZAttention.forward"><code class="docutils literal notranslate"><span class="pre">LiZAttention.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttention"><code class="docutils literal notranslate"><span class="pre">LinearAttention</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttention.forward"><code class="docutils literal notranslate"><span class="pre">LinearAttention.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttentionOp"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttentionOp.chunk_delta_product_forward"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.chunk_delta_product_forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttentionOp.compute_gate"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.compute_gate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttentionOp.forward"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttentionOp.get_cache"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.get_cache()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.LinearAttentionOp.save_cache"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.save_cache()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel"><code class="docutils literal notranslate"><span class="pre">TpttModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel.config_class"><code class="docutils literal notranslate"><span class="pre">TpttModel.config_class</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel.forward"><code class="docutils literal notranslate"><span class="pre">TpttModel.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel.from_pretrained"><code class="docutils literal notranslate"><span class="pre">TpttModel.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel.generate"><code class="docutils literal notranslate"><span class="pre">TpttModel.generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel.inject_liza_attention"><code class="docutils literal notranslate"><span class="pre">TpttModel.inject_liza_attention()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel.retie_lm_after_load"><code class="docutils literal notranslate"><span class="pre">TpttModel.retie_lm_after_load()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.modeling_tptt.TpttModel.save_pretrained"><code class="docutils literal notranslate"><span class="pre">TpttModel.save_pretrained()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.apply_linear_attention_mask"><code class="docutils literal notranslate"><span class="pre">apply_linear_attention_mask()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.chunk_sequence"><code class="docutils literal notranslate"><span class="pre">chunk_sequence()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.describe"><code class="docutils literal notranslate"><span class="pre">describe()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.ensure_stability"><code class="docutils literal notranslate"><span class="pre">ensure_stability()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.expand_virtual_tokens"><code class="docutils literal notranslate"><span class="pre">expand_virtual_tokens()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.extract_layer_idx"><code class="docutils literal notranslate"><span class="pre">extract_layer_idx()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.fast_invert_matrix"><code class="docutils literal notranslate"><span class="pre">fast_invert_matrix()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.find_embedding_lm"><code class="docutils literal notranslate"><span class="pre">find_embedding_lm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.get_tptt_model"><code class="docutils literal notranslate"><span class="pre">get_tptt_model()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.get_valid_chunk_size"><code class="docutils literal notranslate"><span class="pre">get_valid_chunk_size()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.load_tptt_safetensors"><code class="docutils literal notranslate"><span class="pre">load_tptt_safetensors()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.match_dim"><code class="docutils literal notranslate"><span class="pre">match_dim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.sequential_delta_product_scan"><code class="docutils literal notranslate"><span class="pre">sequential_delta_product_scan()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.soft_clamp"><code class="docutils literal notranslate"><span class="pre">soft_clamp()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.split_qkv"><code class="docutils literal notranslate"><span class="pre">split_qkv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.truncate_attention_mask"><code class="docutils literal notranslate"><span class="pre">truncate_attention_mask()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.modeling_tptt.unlinear_activation"><code class="docutils literal notranslate"><span class="pre">unlinear_activation()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-tptt.train_tptt">tptt.train_tptt module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tptt.train_tptt.LiZACallback"><code class="docutils literal notranslate"><span class="pre">LiZACallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.train_tptt.LiZACallback.on_log"><code class="docutils literal notranslate"><span class="pre">LiZACallback.on_log()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.train_tptt.LiZACallback.on_step_end"><code class="docutils literal notranslate"><span class="pre">LiZACallback.on_step_end()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.train_tptt.SaveBestModelCallback"><code class="docutils literal notranslate"><span class="pre">SaveBestModelCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.train_tptt.SaveBestModelCallback.on_evaluate"><code class="docutils literal notranslate"><span class="pre">SaveBestModelCallback.on_evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.train_tptt.ensure_int"><code class="docutils literal notranslate"><span class="pre">ensure_int()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-tptt">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tptt.LCache"><code class="docutils literal notranslate"><span class="pre">LCache</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LCache.reset"><code class="docutils literal notranslate"><span class="pre">LCache.reset()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LCache.update"><code class="docutils literal notranslate"><span class="pre">LCache.update()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.LiZACallback"><code class="docutils literal notranslate"><span class="pre">LiZACallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LiZACallback.on_log"><code class="docutils literal notranslate"><span class="pre">LiZACallback.on_log()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LiZACallback.on_step_end"><code class="docutils literal notranslate"><span class="pre">LiZACallback.on_step_end()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.LiZAttention"><code class="docutils literal notranslate"><span class="pre">LiZAttention</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LiZAttention.forward"><code class="docutils literal notranslate"><span class="pre">LiZAttention.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.LinearAttention"><code class="docutils literal notranslate"><span class="pre">LinearAttention</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LinearAttention.forward"><code class="docutils literal notranslate"><span class="pre">LinearAttention.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.LinearAttentionOp"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LinearAttentionOp.chunk_delta_product_forward"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.chunk_delta_product_forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LinearAttentionOp.compute_gate"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.compute_gate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LinearAttentionOp.forward"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LinearAttentionOp.get_cache"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.get_cache()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.LinearAttentionOp.save_cache"><code class="docutils literal notranslate"><span class="pre">LinearAttentionOp.save_cache()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.SaveBestModelCallback"><code class="docutils literal notranslate"><span class="pre">SaveBestModelCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.SaveBestModelCallback.on_evaluate"><code class="docutils literal notranslate"><span class="pre">SaveBestModelCallback.on_evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.TpttConfig"><code class="docutils literal notranslate"><span class="pre">TpttConfig</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttConfig.RECURRENT_MODES"><code class="docutils literal notranslate"><span class="pre">TpttConfig.RECURRENT_MODES</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttConfig.architectures"><code class="docutils literal notranslate"><span class="pre">TpttConfig.architectures</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttConfig.auto_map"><code class="docutils literal notranslate"><span class="pre">TpttConfig.auto_map</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttConfig.model_type"><code class="docutils literal notranslate"><span class="pre">TpttConfig.model_type</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.TpttModel"><code class="docutils literal notranslate"><span class="pre">TpttModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttModel.config_class"><code class="docutils literal notranslate"><span class="pre">TpttModel.config_class</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttModel.forward"><code class="docutils literal notranslate"><span class="pre">TpttModel.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttModel.from_pretrained"><code class="docutils literal notranslate"><span class="pre">TpttModel.from_pretrained()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttModel.generate"><code class="docutils literal notranslate"><span class="pre">TpttModel.generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttModel.inject_liza_attention"><code class="docutils literal notranslate"><span class="pre">TpttModel.inject_liza_attention()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttModel.retie_lm_after_load"><code class="docutils literal notranslate"><span class="pre">TpttModel.retie_lm_after_load()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tptt.TpttModel.save_pretrained"><code class="docutils literal notranslate"><span class="pre">TpttModel.save_pretrained()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.generate_model_card"><code class="docutils literal notranslate"><span class="pre">generate_model_card()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.get_tptt_model"><code class="docutils literal notranslate"><span class="pre">get_tptt_model()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.load_tptt_safetensors"><code class="docutils literal notranslate"><span class="pre">load_tptt_safetensors()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#tptt.parse_mode_name"><code class="docutils literal notranslate"><span class="pre">parse_mode_name()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TPTT</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">tptt package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tptt.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tptt-package">
<h1>tptt package<a class="headerlink" href="#tptt-package" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-tptt.configuration_tptt">
<span id="tptt-configuration-tptt-module"></span><h2>tptt.configuration_tptt module<a class="headerlink" href="#module-tptt.configuration_tptt" title="Link to this heading"></a></h2>
<p>Author : Fabien FURFARO</p>
<dl class="py class">
<dt class="sig sig-object py" id="tptt.configuration_tptt.TpttConfig">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.configuration_tptt.</span></span><span class="sig-name descname"><span class="pre">TpttConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_model_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PretrainedConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'meta-llama/Llama-3.2-1B'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_modules_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_attn_implementation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'eager'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_self_attn_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_scale_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mag_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'float32'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lora_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.configuration_tptt.TpttConfig" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></p>
<p>Configuration class for the TPTT model.
This class merges the backbone config (e.g., Llama) with custom TPTT parameters,</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.configuration_tptt.TpttConfig.RECURRENT_MODES">
<span class="sig-name descname"><span class="pre">RECURRENT_MODES</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'delta_product':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">2,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_product_c':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">2,</span> <span class="pre">'trick':</span> <span class="pre">'combined'},</span> <span class="pre">'delta_product_r':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">2,</span> <span class="pre">'trick':</span> <span class="pre">'rotative'},</span> <span class="pre">'delta_rule':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_rule_gelu':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">False,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_rule_kv':</span> <span class="pre">{'gate_type':</span> <span class="pre">'kv',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_rule_v':</span> <span class="pre">{'gate_type':</span> <span class="pre">'v',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'}}</span></em><a class="headerlink" href="#tptt.configuration_tptt.TpttConfig.RECURRENT_MODES" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.configuration_tptt.TpttConfig.architectures">
<span class="sig-name descname"><span class="pre">architectures</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['TpttModel']</span></em><a class="headerlink" href="#tptt.configuration_tptt.TpttConfig.architectures" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.configuration_tptt.TpttConfig.auto_map">
<span class="sig-name descname"><span class="pre">auto_map</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'AutoConfig':</span> <span class="pre">'configuration_tptt.TpttConfig',</span> <span class="pre">'AutoModelForCausalLM':</span> <span class="pre">'modeling_tptt.TpttModel'}</span></em><a class="headerlink" href="#tptt.configuration_tptt.TpttConfig.auto_map" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.configuration_tptt.TpttConfig.model_type">
<span class="sig-name descname"><span class="pre">model_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'tptt'</span></em><a class="headerlink" href="#tptt.configuration_tptt.TpttConfig.model_type" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.configuration_tptt.convert_sets_to_lists">
<span class="sig-prename descclassname"><span class="pre">tptt.configuration_tptt.</span></span><span class="sig-name descname"><span class="pre">convert_sets_to_lists</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.configuration_tptt.convert_sets_to_lists" title="Link to this definition"></a></dt>
<dd><p>Convert sets to list for LoRA serialized config</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.configuration_tptt.extract_template_variables">
<span class="sig-prename descclassname"><span class="pre">tptt.configuration_tptt.</span></span><span class="sig-name descname"><span class="pre">extract_template_variables</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">template</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">set</span></span></span><a class="headerlink" href="#tptt.configuration_tptt.extract_template_variables" title="Link to this definition"></a></dt>
<dd><p>Basic extract variable from md template</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.configuration_tptt.generate_model_card">
<span class="sig-prename descclassname"><span class="pre">tptt.configuration_tptt.</span></span><span class="sig-name descname"><span class="pre">generate_model_card</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PretrainedConfig</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.configuration_tptt.generate_model_card" title="Link to this definition"></a></dt>
<dd><p>Generate model card from template and training metadata.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.configuration_tptt.get_mode_name">
<span class="sig-prename descclassname"><span class="pre">tptt.configuration_tptt.</span></span><span class="sig-name descname"><span class="pre">get_mode_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">order</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gate_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'k'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trick</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'derivative'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#tptt.configuration_tptt.get_mode_name" title="Link to this definition"></a></dt>
<dd><p>Get recurrent mode name from parameter</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.configuration_tptt.parse_mode_name">
<span class="sig-prename descclassname"><span class="pre">tptt.configuration_tptt.</span></span><span class="sig-name descname"><span class="pre">parse_mode_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="headerlink" href="#tptt.configuration_tptt.parse_mode_name" title="Link to this definition"></a></dt>
<dd><p>Parse mode to recurrent config</p>
</dd></dl>

</section>
<section id="module-tptt.modeling_tptt">
<span id="tptt-modeling-tptt-module"></span><h2>tptt.modeling_tptt module<a class="headerlink" href="#module-tptt.modeling_tptt" title="Link to this heading"></a></h2>
<p>This module implements the TPTT model with linear attention (LiZA) and LoRA support.
Author : Fabien FURFARO
TPTT : Transforming Pretrained Transformers into Titans (<a class="reference external" href="https://arxiv.org/abs/2506.17671">https://arxiv.org/abs/2506.17671</a>)</p>
<dl class="py class">
<dt class="sig sig-object py" id="tptt.modeling_tptt.CausalAvgPool1d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">CausalAvgPool1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offsets</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(0,</span> <span class="pre">1,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'replicate'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.CausalAvgPool1d" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Causal sliding window average (uniform, no shape loss along sequence)</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.CausalAvgPool1d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.CausalAvgPool1d.forward" title="Link to this definition"></a></dt>
<dd><p>x: [B, S, F] → [B, S, F → output_size]</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LCache">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">LCache</span></span><a class="headerlink" href="#tptt.modeling_tptt.LCache" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Cache for storing intermediate states of linear attention layers.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LCache.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.LCache.reset" title="Link to this definition"></a></dt>
<dd><p>Clear all cached states and reset the token counter</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LCache.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.LCache.update" title="Link to this definition"></a></dt>
<dd><p>Detach all tensors to avoid retaining computation graphs</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LiZAttention">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">LiZAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PretrainedConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_self_attn_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_scale_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mag_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'float32'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'right'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disable_linear_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.LiZAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>LiZA Linear Attention module, mixing linear and vanilla attention.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LiZAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.LiZAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Mix linear and self attention forward</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttention">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">LinearAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_key_value_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_key_value_groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'right'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shared_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Linear multi-head attention layer: [B, S, D] -&gt; [B, S, D]
Projections + gating + efficient linear attention mechanism (TPTT compatible).</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_proj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass for linear attention. Input shape: [B, S, D], output [B, S, D].</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttentionOp">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">LinearAttentionOp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttentionOp" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Base class for linear attention operators.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttentionOp.chunk_delta_product_forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">chunk_delta_product_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trick</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'derivative'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttentionOp.chunk_delta_product_forward" title="Link to this definition"></a></dt>
<dd><p>Chunkwise parallel implementation <a class="reference external" href="https://arxiv.org/abs/2406.06484">https://arxiv.org/abs/2406.06484</a>
For each chunk, processes chunk_size * n_orders steps (virtual tokens) in order.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttentionOp.compute_gate">
<span class="sig-name descname"><span class="pre">compute_gate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttentionOp.compute_gate" title="Link to this definition"></a></dt>
<dd><p>Compute the gating tensor according to the gate_type.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttentionOp.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttentionOp.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass for the attention operator.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttentionOp.get_cache">
<span class="sig-name descname"><span class="pre">get_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttentionOp.get_cache" title="Link to this definition"></a></dt>
<dd><p>Retrieve recurrent state and qkv buffers from the cache.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.LinearAttentionOp.save_cache">
<span class="sig-name descname"><span class="pre">save_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.LinearAttentionOp.save_cache" title="Link to this definition"></a></dt>
<dd><p>Save the recurrent state and qkv buffers to the cache.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">TpttModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig" title="tptt.configuration_tptt.TpttConfig"><span class="pre">TpttConfig</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></p>
<p>TPTT model wrapper with linear attention (LiZA) and LoRA support.
Handles only architecture and weights.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel.config_class">
<span class="sig-name descname"><span class="pre">config_class</span></span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel.config_class" title="Link to this definition"></a></dt>
<dd><p>alias of <a class="reference internal" href="#tptt.configuration_tptt.TpttConfig" title="tptt.configuration_tptt.TpttConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TpttConfig</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass. All arguments are passed to the underlying base model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel.from_pretrained">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel.from_pretrained" title="Link to this definition"></a></dt>
<dd><p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with <cite>model.train()</cite>.</p>
<p>The warning <em>Weights from XXX not initialized from pretrained model</em> means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.</p>
<p>The warning <em>Weights from XXX not used in YYY</em> means that the layer XXX is not used by YYY, therefore those
weights are discarded.</p>
<dl>
<dt>Parameters:</dt><dd><dl>
<dt>pretrained_model_name_or_path (<cite>str</cite> or <cite>os.PathLike</cite>, <em>optional</em>):</dt><dd><p>Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</p></li>
<li><p>A path to a <em>directory</em> containing model weights saved using
[<cite>~PreTrainedModel.save_pretrained</cite>], e.g., <cite>./my_model_directory/</cite>.</p></li>
<li><p>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <cite>./tf_model/model.ckpt.index</cite>). In
this case, <cite>from_tf</cite> should be set to <cite>True</cite> and a configuration object should be provided as
<cite>config</cite> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
<li><p>A path or url to a model folder containing a <em>flax checkpoint file</em> in <em>.msgpack</em> format (e.g,
<cite>./flax_model/</cite> containing <cite>flax_model.msgpack</cite>). In this case, <cite>from_flax</cite> should be set to
<cite>True</cite>.</p></li>
<li><p><cite>None</cite> if you are both providing the configuration and state dictionary (resp. with keyword
arguments <cite>config</cite> and <cite>state_dict</cite>).</p></li>
</ul>
</div></blockquote>
</dd>
<dt>model_args (sequence of positional arguments, <em>optional</em>):</dt><dd><p>All remaining positional arguments will be passed to the underlying model’s <cite>__init__</cite> method.</p>
</dd>
<dt>config (<cite>Union[PretrainedConfig, str, os.PathLike]</cite>, <em>optional</em>):</dt><dd><p>Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>an instance of a class derived from [<cite>PretrainedConfig</cite>],</p></li>
<li><p>a string or path valid as input to [<cite>~PretrainedConfig.from_pretrained</cite>].</p></li>
</ul>
</div></blockquote>
<p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul class="simple">
<li><p>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</p></li>
<li><p>The model was saved using [<cite>~PreTrainedModel.save_pretrained</cite>] and is reloaded by supplying the
save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <cite>pretrained_model_name_or_path</cite> and a
configuration JSON file named <em>config.json</em> is found in the directory.</p></li>
</ul>
</div></blockquote>
</dd>
<dt>state_dict (<cite>dict[str, torch.Tensor]</cite>, <em>optional</em>):</dt><dd><p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using [<cite>~PreTrainedModel.save_pretrained</cite>] and
[<cite>~PreTrainedModel.from_pretrained</cite>] is not a simpler option.</p>
</dd>
<dt>cache_dir (<cite>Union[str, os.PathLike]</cite>, <em>optional</em>):</dt><dd><p>Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p>
</dd>
<dt>from_tf (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Load the model weights from a TensorFlow checkpoint save file (see docstring of
<cite>pretrained_model_name_or_path</cite> argument).</p>
</dd>
<dt>from_flax (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Load the model weights from a Flax checkpoint save file (see docstring of
<cite>pretrained_model_name_or_path</cite> argument).</p>
</dd>
<dt>ignore_mismatched_sizes (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
checkpoint with 3 labels).</p>
</dd>
<dt>force_download (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
</dd>
<dt>resume_download:</dt><dd><p>Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.</p>
</dd>
<dt>proxies (<cite>dict[str, str]</cite>, <em>optional</em>):</dt><dd><p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <cite>{‘http’: ‘foo.bar:3128’,
‘http://hostname’: ‘foo.bar:4012’}</cite>. The proxies are used on each request.</p>
</dd>
<dt>output_loading_info(<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p>
</dd>
<dt>local_files_only(<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to only look at local files (i.e., do not try to download the model).</p>
</dd>
<dt>token (<cite>str</cite> or <cite>bool</cite>, <em>optional</em>):</dt><dd><p>The token to use as HTTP bearer authorization for remote files. If <cite>True</cite>, or not specified, will use
the token generated when running <cite>hf auth login</cite> (stored in <cite>~/.huggingface</cite>).</p>
</dd>
<dt>revision (<cite>str</cite>, <em>optional</em>, defaults to <cite>“main”</cite>):</dt><dd><p>The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <cite>revision</cite> can be any
identifier allowed by git.</p>
<p>&lt;Tip&gt;</p>
<p>To test a pull request you made on the Hub, you can pass <cite>revision=”refs/pr/&lt;pr_number&gt;”</cite>.</p>
<p>&lt;/Tip&gt;</p>
</dd>
<dt>attn_implementation (<cite>str</cite>, <em>optional</em>):</dt><dd><p>The attention implementation to use in the model (if relevant). Can be any of <cite>“eager”</cite> (manual implementation of the attention), <cite>“sdpa”</cite> (using [<cite>F.scaled_dot_product_attention</cite>](<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html">https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html</a>)), <cite>“flash_attention_2”</cite> (using [Dao-AILab/flash-attention](<a class="reference external" href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a>)), or <cite>“flash_attention_3”</cite> (using [Dao-AILab/flash-attention/hopper](<a class="reference external" href="https://github.com/Dao-AILab/flash-attention/tree/main/hopper">https://github.com/Dao-AILab/flash-attention/tree/main/hopper</a>)). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <cite>“eager”</cite> implementation.</p>
</dd>
</dl>
<p>&gt; Parameters for big model inference</p>
<dl>
<dt>torch_dtype (<cite>str</cite> or <cite>torch.dtype</cite>, <em>optional</em>):</dt><dd><p>Override the default <cite>torch.dtype</cite> and load the model under a specific <cite>dtype</cite>. The different options
are:</p>
<ol class="arabic simple">
<li><p><cite>torch.float16</cite> or <cite>torch.bfloat16</cite> or <cite>torch.float</cite>: load in a specified</p></li>
</ol>
<blockquote>
<div><p><cite>dtype</cite>, ignoring the model’s <cite>config.torch_dtype</cite> if one exists. If not specified
- the model will get loaded in <cite>torch.float</cite> (fp32).</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><cite>“auto”</cite> - A <cite>torch_dtype</cite> entry in the <cite>config.json</cite> file of the model will be</p></li>
</ol>
<blockquote>
<div><p>attempted to be used. If this entry isn’t found then next check the <cite>dtype</cite> of the first weight in
the checkpoint that’s of a floating point type and use that as <cite>dtype</cite>. This will load the model
using the <cite>dtype</cite> it was saved in at the end of the training. It can’t be used as an indicator of how
the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>A string that is a valid <cite>torch.dtype</cite>. E.g. “float32” loads the model in <cite>torch.float32</cite>, “float16” loads in <cite>torch.float16</cite> etc.</p></li>
</ol>
<p>&lt;Tip&gt;</p>
<p>For some models the <cite>dtype</cite> they were trained in is unknown - you may try to check the model’s paper or
reach out to the authors and ask them to add this information to the model’s card and to insert the
<cite>torch_dtype</cite> entry in <cite>config.json</cite> on the hub.</p>
<p>&lt;/Tip&gt;</p>
</dd>
<dt>device_map (<cite>str</cite> or <cite>dict[str, Union[int, str, torch.device]]</cite> or <cite>int</cite> or <cite>torch.device</cite>, <em>optional</em>):</dt><dd><p>A map that specifies where each submodule should go. It doesn’t need to be refined to each
parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
same device. If we only pass the device (<em>e.g.</em>, <cite>“cpu”</cite>, <cite>“cuda:1”</cite>, <cite>“mps”</cite>, or a GPU ordinal rank
like <cite>1</cite>) on which the model will be allocated, the device map will map the entire model to this
device. Passing <cite>device_map = 0</cite> means put the whole model on GPU 0.</p>
<p>To have Accelerate compute the most optimized <cite>device_map</cite> automatically, set <cite>device_map=”auto”</cite>. For
more information about each option see [designing a device
map](<a class="reference external" href="https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map">https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map</a>).</p>
</dd>
<dt>max_memory (<cite>Dict</cite>, <em>optional</em>):</dt><dd><p>A dictionary device identifier to maximum memory if using <cite>device_map</cite>. Will default to the maximum memory available for each
GPU and the available CPU RAM if unset.</p>
</dd>
<dt>tp_plan (<cite>str</cite>, <em>optional</em>):</dt><dd><p>A torch tensor parallel plan, see [here](<a class="reference external" href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html">https://pytorch.org/tutorials/intermediate/TP_tutorial.html</a>). Currently, it only accepts
<cite>tp_plan=”auto”</cite> to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
<cite>torchrun [args] script.py</cite>. This will be much faster than using a <cite>device_map</cite>, but has limitations.</p>
</dd>
<dt>tp_size (<cite>str</cite>, <em>optional</em>):</dt><dd><p>A torch tensor parallel degree. If not provided would default to world size.</p>
</dd>
<dt>device_mesh (<cite>torch.distributed.DeviceMesh</cite>, <em>optional</em>):</dt><dd><p>A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
If provided, it has to contain dimension named <cite>“tp”</cite> which will be used for tensor parallelism</p>
</dd>
<dt>offload_folder (<cite>str</cite> or <cite>os.PathLike</cite>, <em>optional</em>):</dt><dd><p>If the <cite>device_map</cite> contains any value <cite>“disk”</cite>, the folder where we will offload weights.</p>
</dd>
<dt>offload_state_dict (<cite>bool</cite>, <em>optional</em>):</dt><dd><p>If <cite>True</cite>, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU
RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to
<cite>True</cite> when there is some disk offload.</p>
</dd>
<dt>offload_buffers (<cite>bool</cite>, <em>optional</em>):</dt><dd><p>Whether or not to offload the buffers with the model parameters.</p>
</dd>
<dt>quantization_config (<cite>Union[QuantizationConfigMixin,Dict]</cite>, <em>optional</em>):</dt><dd><p>A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
bitsandbytes, gptq). There may be other quantization-related kwargs, including <cite>load_in_4bit</cite> and
<cite>load_in_8bit</cite>, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
quantizations and not preferred. consider inserting all such arguments into quantization_config
instead.</p>
</dd>
<dt>subfolder (<cite>str</cite>, <em>optional</em>, defaults to <cite>“”</cite>):</dt><dd><p>In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
specify the folder name here.</p>
</dd>
<dt>variant (<cite>str</cite>, <em>optional</em>):</dt><dd><p>If specified load weights from <cite>variant</cite> filename, <em>e.g.</em> pytorch_model.&lt;variant&gt;.bin. <cite>variant</cite> is
ignored when using <cite>from_tf</cite> or <cite>from_flax</cite>.</p>
</dd>
<dt>use_safetensors (<cite>bool</cite>, <em>optional</em>, defaults to <cite>None</cite>):</dt><dd><p>Whether or not to use <cite>safetensors</cite> checkpoints. Defaults to <cite>None</cite>. If not specified and <cite>safetensors</cite>
is not installed, it will be set to <cite>False</cite>.</p>
</dd>
<dt>weights_only (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>):</dt><dd><p>Indicates whether unpickler should be restricted to loading only tensors, primitive types,
dictionaries and any types added via torch.serialization.add_safe_globals().
When set to False, we can load wrapper tensor subclass weights.</p>
</dd>
<dt>key_mapping (<a href="#id1"><span class="problematic" id="id2">`</span></a>dict[str, str], <em>optional</em>):</dt><dd><p>A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
architecture, but was not converted accordingly.</p>
</dd>
<dt>kwargs (remaining dictionary of keyword arguments, <em>optional</em>):</dt><dd><p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<cite>output_attentions=True</cite>). Behaves differently depending on whether a <cite>config</cite> is provided or
automatically loaded:</p>
<blockquote>
<div><ul class="simple">
<li><p>If a configuration is provided with <cite>config</cite>, <cite>**kwargs</cite> will be directly passed to the
underlying model’s <cite>__init__</cite> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <cite>kwargs</cite> will be first passed to the configuration class
initialization function ([<cite>~PretrainedConfig.from_pretrained</cite>]). Each key of <cite>kwargs</cite> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <cite>kwargs</cite> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model’s <cite>__init__</cite> function.</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<p>&lt;Tip&gt;</p>
<p>Activate the special [“offline-mode”](<a class="reference external" href="https://huggingface.co/transformers/installation.html#offline-mode">https://huggingface.co/transformers/installation.html#offline-mode</a>) to
use this method in a firewalled environment.</p>
<p>&lt;/Tip&gt;</p>
<p>Examples:</p>
<p><a href="#id3"><span class="problematic" id="id4">``</span></a><a href="#id5"><span class="problematic" id="id6">`</span></a>python
&gt;&gt;&gt; from transformers import BertConfig, BertModel</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Model was saved using *save_pretrained(&#39;./test/saved_model/&#39;)* (for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_model_config.json&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_checkpoint.ckpt.index&quot;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a Flax checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">from_flax</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel.generate" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel.inject_liza_attention">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">inject_liza_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backbone</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig" title="tptt.configuration_tptt.TpttConfig"><span class="pre">TpttConfig</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">PreTrainedModel</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel.inject_liza_attention" title="Link to this definition"></a></dt>
<dd><p>Inject LiZAttention into the specified target modules of the base model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel.retie_lm_after_load">
<span class="sig-name descname"><span class="pre">retie_lm_after_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel.retie_lm_after_load" title="Link to this definition"></a></dt>
<dd><p>Re-link lm_head after loading external weights.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.modeling_tptt.TpttModel.save_pretrained">
<span class="sig-name descname"><span class="pre">save_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.modeling_tptt.TpttModel.save_pretrained" title="Link to this definition"></a></dt>
<dd><p>Save model weights, config, and source code to the given path.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.apply_linear_attention_mask">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">apply_linear_attention_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'right'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.apply_linear_attention_mask" title="Link to this definition"></a></dt>
<dd><p>Extract if padding –&gt; [B,S]</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.chunk_sequence">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">chunk_sequence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_chunks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.chunk_sequence" title="Link to this definition"></a></dt>
<dd><p>Splits [B, H, S, D] to  [B, H, num_chunks, chunk_size, D]</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.describe">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">describe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tensor'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.describe" title="Link to this definition"></a></dt>
<dd><p>Prints the shape, min, max, mean, and std of a tensor.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.ensure_stability">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">ensure_stability</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-10000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10000.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.ensure_stability" title="Link to this definition"></a></dt>
<dd><p>stability forcing</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.expand_virtual_tokens">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">expand_virtual_tokens</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'derivative'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.expand_virtual_tokens" title="Link to this definition"></a></dt>
<dd><p>Expand tokens into ‘n’ virtual tokens using the selected trick.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.extract_layer_idx">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">extract_layer_idx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.extract_layer_idx" title="Link to this definition"></a></dt>
<dd><p>Extract the layer index from a module name string.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.fast_invert_matrix">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">fast_invert_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tri_tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.fast_invert_matrix" title="Link to this definition"></a></dt>
<dd><p>Equivalent to vectorized forward substitution applied to the identity matrix.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.find_embedding_lm">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">find_embedding_lm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.find_embedding_lm" title="Link to this definition"></a></dt>
<dd><p>Find the embedding weight in a model module.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.get_tptt_model">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">get_tptt_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_config:</span> <span class="pre">~transformers.configuration_utils.PretrainedConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">liza_attention:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'tptt.modeling_tptt.LiZAttention'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_modules:</span> <span class="pre">list[str]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache:</span> <span class="pre">~tptt.modeling_tptt.LCache</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config:</span> <span class="pre">~typing.Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.Any]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_scale_attn:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mag_weight:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_gate:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision:</span> <span class="pre">~torch.dtype</span> <span class="pre">=</span> <span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_self_attn_length:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'right'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">PreTrainedModel</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.modeling_tptt.get_tptt_model" title="Link to this definition"></a></dt>
<dd><p>Replace target modules in a model with LiZAttention.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.get_valid_chunk_size">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">get_valid_chunk_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">total_l</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.get_valid_chunk_size" title="Link to this definition"></a></dt>
<dd><p>Return the largest chunk_size &lt;= chunk_size that divides total_l.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.load_tptt_safetensors">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">load_tptt_safetensors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repo_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PeftModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">PeftModel</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.load_tptt_safetensors" title="Link to this definition"></a></dt>
<dd><p>Load Tptt safetensor from LoRA/PEFT weights and adapt keys if needed.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.match_dim">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">match_dim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.match_dim" title="Link to this definition"></a></dt>
<dd><p>Match the size of tensor x along dimension dim to target_size by interpolation</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.sequential_delta_product_scan">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">sequential_delta_product_scan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_chunks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">u</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_orders</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_activation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">current_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_recurrent_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.modeling_tptt.sequential_delta_product_scan" title="Link to this definition"></a></dt>
<dd><p>DeltaProduct implementation <a class="reference external" href="https://arxiv.org/abs/2502.10297">https://arxiv.org/abs/2502.10297</a>
Implements the per-token Householder state updates.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.soft_clamp">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">soft_clamp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.999999</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.soft_clamp" title="Link to this definition"></a></dt>
<dd><p>Differentiable clamping for stability</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.split_qkv">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">split_qkv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qkv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.modeling_tptt.split_qkv" title="Link to this definition"></a></dt>
<dd><p>Split the QKV tensor into separate Q, K, and V tensors.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.truncate_attention_mask">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">truncate_attention_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.modeling_tptt.truncate_attention_mask" title="Link to this definition"></a></dt>
<dd><p>Truncate hidden_states and attention_mask to the last window of size max_length</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.modeling_tptt.unlinear_activation">
<span class="sig-prename descclassname"><span class="pre">tptt.modeling_tptt.</span></span><span class="sig-name descname"><span class="pre">unlinear_activation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2.0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.modeling_tptt.unlinear_activation" title="Link to this definition"></a></dt>
<dd><p>Unlinear activation between chunk</p>
</dd></dl>

</section>
<section id="module-tptt.train_tptt">
<span id="tptt-train-tptt-module"></span><h2>tptt.train_tptt module<a class="headerlink" href="#module-tptt.train_tptt" title="Link to this heading"></a></h2>
<p>Author : Fabien FURFARO</p>
<dl class="py class">
<dt class="sig sig-object py" id="tptt.train_tptt.LiZACallback">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.train_tptt.</span></span><span class="sig-name descname"><span class="pre">LiZACallback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'gradual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transition_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">switch_period</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.train_tptt.LiZACallback" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></p>
<p>TrainerCallback to schedule mag_weight or enable/disable linear attention during training.</p>
<dl class="simple">
<dt>Modes:</dt><dd><ul class="simple">
<li><p>“gradual”: linear interpolation from initial_weight to final_weight.</p></li>
<li><p>“cyclic”: alternate between values in weight_list at each step.</p></li>
<li><p>“switch”: alternately enable/disable linear attention at each step.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.train_tptt.LiZACallback.on_log">
<span class="sig-name descname"><span class="pre">on_log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">control</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.train_tptt.LiZACallback.on_log" title="Link to this definition"></a></dt>
<dd><p>Event called after logging the last logs.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.train_tptt.LiZACallback.on_step_end">
<span class="sig-name descname"><span class="pre">on_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">control</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.train_tptt.LiZACallback.on_step_end" title="Link to this definition"></a></dt>
<dd><p>Event called at the end of a training step. If using gradient accumulation, one training step might take
several inputs.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.train_tptt.SaveBestModelCallback">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.train_tptt.</span></span><span class="sig-name descname"><span class="pre">SaveBestModelCallback</span></span><a class="headerlink" href="#tptt.train_tptt.SaveBestModelCallback" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></p>
<p>TrainerCallback to save the best model based on evaluation loss.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.train_tptt.SaveBestModelCallback.on_evaluate">
<span class="sig-name descname"><span class="pre">on_evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">control</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.train_tptt.SaveBestModelCallback.on_evaluate" title="Link to this definition"></a></dt>
<dd><p>Event called after an evaluation phase.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.train_tptt.ensure_int">
<span class="sig-prename descclassname"><span class="pre">tptt.train_tptt.</span></span><span class="sig-name descname"><span class="pre">ensure_int</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#tptt.train_tptt.ensure_int" title="Link to this definition"></a></dt>
<dd><p>Ensure the value is a plain integer.</p>
</dd></dl>

</section>
<section id="module-tptt">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-tptt" title="Link to this heading"></a></h2>
<p>This module implements the TPTT model with linear attention (LiZA) and LoRA support.</p>
<dl class="py class">
<dt class="sig sig-object py" id="tptt.LCache">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">LCache</span></span><a class="headerlink" href="#tptt.LCache" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Cache for storing intermediate states of linear attention layers.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.LCache.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LCache.reset" title="Link to this definition"></a></dt>
<dd><p>Clear all cached states and reset the token counter</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.LCache.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LCache.update" title="Link to this definition"></a></dt>
<dd><p>Detach all tensors to avoid retaining computation graphs</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.LiZACallback">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">LiZACallback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'gradual'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transition_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_list</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">switch_period</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LiZACallback" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></p>
<p>TrainerCallback to schedule mag_weight or enable/disable linear attention during training.</p>
<dl class="simple">
<dt>Modes:</dt><dd><ul class="simple">
<li><p>“gradual”: linear interpolation from initial_weight to final_weight.</p></li>
<li><p>“cyclic”: alternate between values in weight_list at each step.</p></li>
<li><p>“switch”: alternately enable/disable linear attention at each step.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.LiZACallback.on_log">
<span class="sig-name descname"><span class="pre">on_log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">control</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LiZACallback.on_log" title="Link to this definition"></a></dt>
<dd><p>Event called after logging the last logs.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.LiZACallback.on_step_end">
<span class="sig-name descname"><span class="pre">on_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">control</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LiZACallback.on_step_end" title="Link to this definition"></a></dt>
<dd><p>Event called at the end of a training step. If using gradient accumulation, one training step might take
several inputs.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.LiZAttention">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">LiZAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PretrainedConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_self_attn_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_scale_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mag_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'float32'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'right'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disable_linear_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LiZAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>LiZA Linear Attention module, mixing linear and vanilla attention.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.LiZAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.LiZAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Mix linear and self attention forward</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.LinearAttention">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">LinearAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_key_value_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_key_value_groups</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'right'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shared_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LinearAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Linear multi-head attention layer: [B, S, D] -&gt; [B, S, D]
Projections + gating + efficient linear attention mechanism (TPTT compatible).</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.LinearAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_proj</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.LinearAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass for linear attention. Input shape: [B, S, D], output [B, S, D].</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.LinearAttentionOp">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">LinearAttentionOp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.LinearAttentionOp" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Base class for linear attention operators.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.LinearAttentionOp.chunk_delta_product_forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">chunk_delta_product_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trick</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'derivative'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.float32</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.LinearAttentionOp.chunk_delta_product_forward" title="Link to this definition"></a></dt>
<dd><p>Chunkwise parallel implementation <a class="reference external" href="https://arxiv.org/abs/2406.06484">https://arxiv.org/abs/2406.06484</a>
For each chunk, processes chunk_size * n_orders steps (virtual tokens) in order.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.LinearAttentionOp.compute_gate">
<span class="sig-name descname"><span class="pre">compute_gate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.LinearAttentionOp.compute_gate" title="Link to this definition"></a></dt>
<dd><p>Compute the gating tensor according to the gate_type.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.LinearAttentionOp.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#tptt.LinearAttentionOp.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass for the attention operator.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.LinearAttentionOp.get_cache">
<span class="sig-name descname"><span class="pre">get_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.LinearAttentionOp.get_cache" title="Link to this definition"></a></dt>
<dd><p>Retrieve recurrent state and qkv buffers from the cache.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.LinearAttentionOp.save_cache">
<span class="sig-name descname"><span class="pre">save_cache</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.LinearAttentionOp.save_cache" title="Link to this definition"></a></dt>
<dd><p>Save the recurrent state and qkv buffers to the cache.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.SaveBestModelCallback">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">SaveBestModelCallback</span></span><a class="headerlink" href="#tptt.SaveBestModelCallback" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">TrainerCallback</span></code></p>
<p>TrainerCallback to save the best model based on evaluation loss.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tptt.SaveBestModelCallback.on_evaluate">
<span class="sig-name descname"><span class="pre">on_evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">control</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.SaveBestModelCallback.on_evaluate" title="Link to this definition"></a></dt>
<dd><p>Event called after an evaluation phase.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.TpttConfig">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">TpttConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_model_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">PretrainedConfig</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'meta-llama/Llama-3.2-1B'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_modules_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_attn_implementation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'eager'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_self_attn_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_scale_attn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mag_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_gate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'float32'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lora_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.TpttConfig" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code></p>
<p>Configuration class for the TPTT model.
This class merges the backbone config (e.g., Llama) with custom TPTT parameters,</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.TpttConfig.RECURRENT_MODES">
<span class="sig-name descname"><span class="pre">RECURRENT_MODES</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'delta_product':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">2,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_product_c':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">2,</span> <span class="pre">'trick':</span> <span class="pre">'combined'},</span> <span class="pre">'delta_product_r':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">2,</span> <span class="pre">'trick':</span> <span class="pre">'rotative'},</span> <span class="pre">'delta_rule':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_rule_gelu':</span> <span class="pre">{'gate_type':</span> <span class="pre">'k',</span> <span class="pre">'linear':</span> <span class="pre">False,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_rule_kv':</span> <span class="pre">{'gate_type':</span> <span class="pre">'kv',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'},</span> <span class="pre">'delta_rule_v':</span> <span class="pre">{'gate_type':</span> <span class="pre">'v',</span> <span class="pre">'linear':</span> <span class="pre">True,</span> <span class="pre">'order':</span> <span class="pre">1,</span> <span class="pre">'trick':</span> <span class="pre">'derivative'}}</span></em><a class="headerlink" href="#tptt.TpttConfig.RECURRENT_MODES" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.TpttConfig.architectures">
<span class="sig-name descname"><span class="pre">architectures</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['TpttModel']</span></em><a class="headerlink" href="#tptt.TpttConfig.architectures" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.TpttConfig.auto_map">
<span class="sig-name descname"><span class="pre">auto_map</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">{'AutoConfig':</span> <span class="pre">'configuration_tptt.TpttConfig',</span> <span class="pre">'AutoModelForCausalLM':</span> <span class="pre">'modeling_tptt.TpttModel'}</span></em><a class="headerlink" href="#tptt.TpttConfig.auto_map" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.TpttConfig.model_type">
<span class="sig-name descname"><span class="pre">model_type</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">str</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'tptt'</span></em><a class="headerlink" href="#tptt.TpttConfig.model_type" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tptt.TpttModel">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">TpttModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig" title="tptt.configuration_tptt.TpttConfig"><span class="pre">TpttConfig</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.TpttModel" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code></p>
<p>TPTT model wrapper with linear attention (LiZA) and LoRA support.
Handles only architecture and weights.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="tptt.TpttModel.config_class">
<span class="sig-name descname"><span class="pre">config_class</span></span><a class="headerlink" href="#tptt.TpttModel.config_class" title="Link to this definition"></a></dt>
<dd><p>alias of <a class="reference internal" href="#tptt.configuration_tptt.TpttConfig" title="tptt.configuration_tptt.TpttConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">TpttConfig</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.TpttModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LongTensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.TpttModel.forward" title="Link to this definition"></a></dt>
<dd><p>Forward pass. All arguments are passed to the underlying base model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.TpttModel.from_pretrained">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.TpttModel.from_pretrained" title="Link to this definition"></a></dt>
<dd><p>Instantiate a pretrained pytorch model from a pre-trained model configuration.</p>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with <cite>model.train()</cite>.</p>
<p>The warning <em>Weights from XXX not initialized from pretrained model</em> means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.</p>
<p>The warning <em>Weights from XXX not used in YYY</em> means that the layer XXX is not used by YYY, therefore those
weights are discarded.</p>
<dl>
<dt>Parameters:</dt><dd><dl>
<dt>pretrained_model_name_or_path (<cite>str</cite> or <cite>os.PathLike</cite>, <em>optional</em>):</dt><dd><p>Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</p></li>
<li><p>A path to a <em>directory</em> containing model weights saved using
[<cite>~PreTrainedModel.save_pretrained</cite>], e.g., <cite>./my_model_directory/</cite>.</p></li>
<li><p>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <cite>./tf_model/model.ckpt.index</cite>). In
this case, <cite>from_tf</cite> should be set to <cite>True</cite> and a configuration object should be provided as
<cite>config</cite> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
<li><p>A path or url to a model folder containing a <em>flax checkpoint file</em> in <em>.msgpack</em> format (e.g,
<cite>./flax_model/</cite> containing <cite>flax_model.msgpack</cite>). In this case, <cite>from_flax</cite> should be set to
<cite>True</cite>.</p></li>
<li><p><cite>None</cite> if you are both providing the configuration and state dictionary (resp. with keyword
arguments <cite>config</cite> and <cite>state_dict</cite>).</p></li>
</ul>
</div></blockquote>
</dd>
<dt>model_args (sequence of positional arguments, <em>optional</em>):</dt><dd><p>All remaining positional arguments will be passed to the underlying model’s <cite>__init__</cite> method.</p>
</dd>
<dt>config (<cite>Union[PretrainedConfig, str, os.PathLike]</cite>, <em>optional</em>):</dt><dd><p>Can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>an instance of a class derived from [<cite>PretrainedConfig</cite>],</p></li>
<li><p>a string or path valid as input to [<cite>~PretrainedConfig.from_pretrained</cite>].</p></li>
</ul>
</div></blockquote>
<p>Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<blockquote>
<div><ul class="simple">
<li><p>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</p></li>
<li><p>The model was saved using [<cite>~PreTrainedModel.save_pretrained</cite>] and is reloaded by supplying the
save directory.</p></li>
<li><p>The model is loaded by supplying a local directory as <cite>pretrained_model_name_or_path</cite> and a
configuration JSON file named <em>config.json</em> is found in the directory.</p></li>
</ul>
</div></blockquote>
</dd>
<dt>state_dict (<cite>dict[str, torch.Tensor]</cite>, <em>optional</em>):</dt><dd><p>A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using [<cite>~PreTrainedModel.save_pretrained</cite>] and
[<cite>~PreTrainedModel.from_pretrained</cite>] is not a simpler option.</p>
</dd>
<dt>cache_dir (<cite>Union[str, os.PathLike]</cite>, <em>optional</em>):</dt><dd><p>Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.</p>
</dd>
<dt>from_tf (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Load the model weights from a TensorFlow checkpoint save file (see docstring of
<cite>pretrained_model_name_or_path</cite> argument).</p>
</dd>
<dt>from_flax (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Load the model weights from a Flax checkpoint save file (see docstring of
<cite>pretrained_model_name_or_path</cite> argument).</p>
</dd>
<dt>ignore_mismatched_sizes (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to raise an error if some of the weights from the checkpoint do not have the same size
as the weights of the model (if for instance, you are instantiating a model with 10 labels from a
checkpoint with 3 labels).</p>
</dd>
<dt>force_download (<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.</p>
</dd>
<dt>resume_download:</dt><dd><p>Deprecated and ignored. All downloads are now resumed by default when possible.
Will be removed in v5 of Transformers.</p>
</dd>
<dt>proxies (<cite>dict[str, str]</cite>, <em>optional</em>):</dt><dd><p>A dictionary of proxy servers to use by protocol or endpoint, e.g., <cite>{‘http’: ‘foo.bar:3128’,
‘http://hostname’: ‘foo.bar:4012’}</cite>. The proxies are used on each request.</p>
</dd>
<dt>output_loading_info(<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.</p>
</dd>
<dt>local_files_only(<cite>bool</cite>, <em>optional</em>, defaults to <cite>False</cite>):</dt><dd><p>Whether or not to only look at local files (i.e., do not try to download the model).</p>
</dd>
<dt>token (<cite>str</cite> or <cite>bool</cite>, <em>optional</em>):</dt><dd><p>The token to use as HTTP bearer authorization for remote files. If <cite>True</cite>, or not specified, will use
the token generated when running <cite>hf auth login</cite> (stored in <cite>~/.huggingface</cite>).</p>
</dd>
<dt>revision (<cite>str</cite>, <em>optional</em>, defaults to <cite>“main”</cite>):</dt><dd><p>The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <cite>revision</cite> can be any
identifier allowed by git.</p>
<p>&lt;Tip&gt;</p>
<p>To test a pull request you made on the Hub, you can pass <cite>revision=”refs/pr/&lt;pr_number&gt;”</cite>.</p>
<p>&lt;/Tip&gt;</p>
</dd>
<dt>attn_implementation (<cite>str</cite>, <em>optional</em>):</dt><dd><p>The attention implementation to use in the model (if relevant). Can be any of <cite>“eager”</cite> (manual implementation of the attention), <cite>“sdpa”</cite> (using [<cite>F.scaled_dot_product_attention</cite>](<a class="reference external" href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html">https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html</a>)), <cite>“flash_attention_2”</cite> (using [Dao-AILab/flash-attention](<a class="reference external" href="https://github.com/Dao-AILab/flash-attention">https://github.com/Dao-AILab/flash-attention</a>)), or <cite>“flash_attention_3”</cite> (using [Dao-AILab/flash-attention/hopper](<a class="reference external" href="https://github.com/Dao-AILab/flash-attention/tree/main/hopper">https://github.com/Dao-AILab/flash-attention/tree/main/hopper</a>)). By default, if available, SDPA will be used for torch&gt;=2.1.1. The default is otherwise the manual <cite>“eager”</cite> implementation.</p>
</dd>
</dl>
<p>&gt; Parameters for big model inference</p>
<dl>
<dt>torch_dtype (<cite>str</cite> or <cite>torch.dtype</cite>, <em>optional</em>):</dt><dd><p>Override the default <cite>torch.dtype</cite> and load the model under a specific <cite>dtype</cite>. The different options
are:</p>
<ol class="arabic simple">
<li><p><cite>torch.float16</cite> or <cite>torch.bfloat16</cite> or <cite>torch.float</cite>: load in a specified</p></li>
</ol>
<blockquote>
<div><p><cite>dtype</cite>, ignoring the model’s <cite>config.torch_dtype</cite> if one exists. If not specified
- the model will get loaded in <cite>torch.float</cite> (fp32).</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><cite>“auto”</cite> - A <cite>torch_dtype</cite> entry in the <cite>config.json</cite> file of the model will be</p></li>
</ol>
<blockquote>
<div><p>attempted to be used. If this entry isn’t found then next check the <cite>dtype</cite> of the first weight in
the checkpoint that’s of a floating point type and use that as <cite>dtype</cite>. This will load the model
using the <cite>dtype</cite> it was saved in at the end of the training. It can’t be used as an indicator of how
the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>A string that is a valid <cite>torch.dtype</cite>. E.g. “float32” loads the model in <cite>torch.float32</cite>, “float16” loads in <cite>torch.float16</cite> etc.</p></li>
</ol>
<p>&lt;Tip&gt;</p>
<p>For some models the <cite>dtype</cite> they were trained in is unknown - you may try to check the model’s paper or
reach out to the authors and ask them to add this information to the model’s card and to insert the
<cite>torch_dtype</cite> entry in <cite>config.json</cite> on the hub.</p>
<p>&lt;/Tip&gt;</p>
</dd>
<dt>device_map (<cite>str</cite> or <cite>dict[str, Union[int, str, torch.device]]</cite> or <cite>int</cite> or <cite>torch.device</cite>, <em>optional</em>):</dt><dd><p>A map that specifies where each submodule should go. It doesn’t need to be refined to each
parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
same device. If we only pass the device (<em>e.g.</em>, <cite>“cpu”</cite>, <cite>“cuda:1”</cite>, <cite>“mps”</cite>, or a GPU ordinal rank
like <cite>1</cite>) on which the model will be allocated, the device map will map the entire model to this
device. Passing <cite>device_map = 0</cite> means put the whole model on GPU 0.</p>
<p>To have Accelerate compute the most optimized <cite>device_map</cite> automatically, set <cite>device_map=”auto”</cite>. For
more information about each option see [designing a device
map](<a class="reference external" href="https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map">https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map</a>).</p>
</dd>
<dt>max_memory (<cite>Dict</cite>, <em>optional</em>):</dt><dd><p>A dictionary device identifier to maximum memory if using <cite>device_map</cite>. Will default to the maximum memory available for each
GPU and the available CPU RAM if unset.</p>
</dd>
<dt>tp_plan (<cite>str</cite>, <em>optional</em>):</dt><dd><p>A torch tensor parallel plan, see [here](<a class="reference external" href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html">https://pytorch.org/tutorials/intermediate/TP_tutorial.html</a>). Currently, it only accepts
<cite>tp_plan=”auto”</cite> to use predefined plan based on the model. Note that if you use it, you should launch your script accordingly with
<cite>torchrun [args] script.py</cite>. This will be much faster than using a <cite>device_map</cite>, but has limitations.</p>
</dd>
<dt>tp_size (<cite>str</cite>, <em>optional</em>):</dt><dd><p>A torch tensor parallel degree. If not provided would default to world size.</p>
</dd>
<dt>device_mesh (<cite>torch.distributed.DeviceMesh</cite>, <em>optional</em>):</dt><dd><p>A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.
If provided, it has to contain dimension named <cite>“tp”</cite> which will be used for tensor parallelism</p>
</dd>
<dt>offload_folder (<cite>str</cite> or <cite>os.PathLike</cite>, <em>optional</em>):</dt><dd><p>If the <cite>device_map</cite> contains any value <cite>“disk”</cite>, the folder where we will offload weights.</p>
</dd>
<dt>offload_state_dict (<cite>bool</cite>, <em>optional</em>):</dt><dd><p>If <cite>True</cite>, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU
RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to
<cite>True</cite> when there is some disk offload.</p>
</dd>
<dt>offload_buffers (<cite>bool</cite>, <em>optional</em>):</dt><dd><p>Whether or not to offload the buffers with the model parameters.</p>
</dd>
<dt>quantization_config (<cite>Union[QuantizationConfigMixin,Dict]</cite>, <em>optional</em>):</dt><dd><p>A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g
bitsandbytes, gptq). There may be other quantization-related kwargs, including <cite>load_in_4bit</cite> and
<cite>load_in_8bit</cite>, which are parsed by QuantizationConfigParser. Supported only for bitsandbytes
quantizations and not preferred. consider inserting all such arguments into quantization_config
instead.</p>
</dd>
<dt>subfolder (<cite>str</cite>, <em>optional</em>, defaults to <cite>“”</cite>):</dt><dd><p>In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
specify the folder name here.</p>
</dd>
<dt>variant (<cite>str</cite>, <em>optional</em>):</dt><dd><p>If specified load weights from <cite>variant</cite> filename, <em>e.g.</em> pytorch_model.&lt;variant&gt;.bin. <cite>variant</cite> is
ignored when using <cite>from_tf</cite> or <cite>from_flax</cite>.</p>
</dd>
<dt>use_safetensors (<cite>bool</cite>, <em>optional</em>, defaults to <cite>None</cite>):</dt><dd><p>Whether or not to use <cite>safetensors</cite> checkpoints. Defaults to <cite>None</cite>. If not specified and <cite>safetensors</cite>
is not installed, it will be set to <cite>False</cite>.</p>
</dd>
<dt>weights_only (<cite>bool</cite>, <em>optional</em>, defaults to <cite>True</cite>):</dt><dd><p>Indicates whether unpickler should be restricted to loading only tensors, primitive types,
dictionaries and any types added via torch.serialization.add_safe_globals().
When set to False, we can load wrapper tensor subclass weights.</p>
</dd>
<dt>key_mapping (<a href="#id7"><span class="problematic" id="id8">`</span></a>dict[str, str], <em>optional</em>):</dt><dd><p>A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers
architecture, but was not converted accordingly.</p>
</dd>
<dt>kwargs (remaining dictionary of keyword arguments, <em>optional</em>):</dt><dd><p>Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<cite>output_attentions=True</cite>). Behaves differently depending on whether a <cite>config</cite> is provided or
automatically loaded:</p>
<blockquote>
<div><ul class="simple">
<li><p>If a configuration is provided with <cite>config</cite>, <cite>**kwargs</cite> will be directly passed to the
underlying model’s <cite>__init__</cite> method (we assume all relevant updates to the configuration have
already been done)</p></li>
<li><p>If a configuration is not provided, <cite>kwargs</cite> will be first passed to the configuration class
initialization function ([<cite>~PretrainedConfig.from_pretrained</cite>]). Each key of <cite>kwargs</cite> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <cite>kwargs</cite> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model’s <cite>__init__</cite> function.</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<p>&lt;Tip&gt;</p>
<p>Activate the special [“offline-mode”](<a class="reference external" href="https://huggingface.co/transformers/installation.html#offline-mode">https://huggingface.co/transformers/installation.html#offline-mode</a>) to
use this method in a firewalled environment.</p>
<p>&lt;/Tip&gt;</p>
<p>Examples:</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python
&gt;&gt;&gt; from transformers import BertConfig, BertModel</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Download model and configuration from huggingface.co and cache.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Model was saved using *save_pretrained(&#39;./test/saved_model/&#39;)* (for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update configuration during loading.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span> <span class="o">==</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower, for example purposes, not runnable).</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_model_config.json&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./tf_model/my_tf_checkpoint.ckpt.index&quot;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading from a Flax checkpoint file instead of a PyTorch model (slower)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">,</span> <span class="n">from_flax</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">```</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.TpttModel.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.TpttModel.generate" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.TpttModel.inject_liza_attention">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">inject_liza_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backbone</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PreTrainedModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.configuration_tptt.TpttConfig" title="tptt.configuration_tptt.TpttConfig"><span class="pre">TpttConfig</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">PreTrainedModel</span></span></span><a class="headerlink" href="#tptt.TpttModel.inject_liza_attention" title="Link to this definition"></a></dt>
<dd><p>Inject LiZAttention into the specified target modules of the base model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.TpttModel.retie_lm_after_load">
<span class="sig-name descname"><span class="pre">retie_lm_after_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.TpttModel.retie_lm_after_load" title="Link to this definition"></a></dt>
<dd><p>Re-link lm_head after loading external weights.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tptt.TpttModel.save_pretrained">
<span class="sig-name descname"><span class="pre">save_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#tptt.TpttModel.save_pretrained" title="Link to this definition"></a></dt>
<dd><p>Save model weights, config, and source code to the given path.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.generate_model_card">
<span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">generate_model_card</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PretrainedConfig</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.generate_model_card" title="Link to this definition"></a></dt>
<dd><p>Generate model card from template and training metadata.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.get_tptt_model">
<span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">get_tptt_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_config:</span> <span class="pre">~transformers.configuration_utils.PretrainedConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">liza_attention:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'tptt.modeling_tptt.LiZAttention'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_modules:</span> <span class="pre">list[str]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_cache:</span> <span class="pre">~tptt.modeling_tptt.LCache</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">operator_mode:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'delta_rule'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurrent_config:</span> <span class="pre">~typing.Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.Any]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_scale_attn:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mag_weight:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cross_gate:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_chunk_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_precision:</span> <span class="pre">~torch.dtype</span> <span class="pre">=</span> <span class="pre">torch.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_self_attn_length:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_side:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'right'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">PreTrainedModel</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference internal" href="#tptt.modeling_tptt.LCache" title="tptt.modeling_tptt.LCache"><span class="pre">LCache</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#tptt.get_tptt_model" title="Link to this definition"></a></dt>
<dd><p>Replace target modules in a model with LiZAttention.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.load_tptt_safetensors">
<span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">load_tptt_safetensors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">repo_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">PeftModel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">PeftModel</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></span><a class="headerlink" href="#tptt.load_tptt_safetensors" title="Link to this definition"></a></dt>
<dd><p>Load Tptt safetensor from LoRA/PEFT weights and adapt keys if needed.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tptt.parse_mode_name">
<span class="sig-prename descclassname"><span class="pre">tptt.</span></span><span class="sig-name descname"><span class="pre">parse_mode_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span></span></span><a class="headerlink" href="#tptt.parse_mode_name" title="Link to this definition"></a></dt>
<dd><p>Parse mode to recurrent config</p>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="TPTT Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Furfaro Fabien.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>