# Titanesque config
dataset: "alpaca-cleaned"
dataset_percentage: 0.10 # ~5000 samples
padding: "longest"
max_length: 386
seed: 42
eval_samples: 2
test_samples: 2
run_test_eval: true
lora: true
batch_size: "auto"
tokenizer: "auto"
log_metrics: true # how to get ?
platform: "Kaggle"
hardware_target: "2xT4"
operator_modes: "delta_product"
linear_precision: "bfloat16"
cross_gate_mode: false
save_model_dir: "./Titans-v2"

# LoRA config flattened
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: [] # empty means default set ["q_proj", "k_proj", "v_proj", "o_proj"]

# Training config flattened: only one active config (standard)
training_epochs: 1
training_learning_rate: 5e-4
training_weight_decay: 0.0
training_gradient_accumulation_steps: 1
training_max_grad_norm: 1.0
training_logging_steps: 5
training_save_steps: 1000
training_save_total_limit: 2
training_bf16: true
training_evaluation_strategy: "steps"
training_report_to: "tensorboard"

# Selected model info (you can override for different runs)
model_name: "meta-llama/Llama-3.2-1B"
model_type: "causal"
model_quantized: false
model_bidirectional: false
model_tokenizer: "" # empty means use model_name tokenizer
model_padding_side: "right" # "left" or "right"

# Liza callback
liza_callback_mode: "constant" # "gradual", "cyclic", "switch"
liza_initial_weight: 0.
liza_final_weight: 0.5
liza_transition_steps: 100
liza_weight_list: [0.0, 0.5, 1.0]
liza_switch_period: 1
