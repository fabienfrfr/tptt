{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e22716b",
   "metadata": {},
   "source": [
    "# ðŸ§  Titanesque Experiment â€“ V2 Training Plan\n",
    "\n",
    "**Purpose:**  \n",
    "This document is a draft requirements note for experiments described in paper **V2**.  \n",
    "It will guide the construction of multiple training configurations from concrete examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Models to Evaluate (with Rationale)\n",
    "\n",
    "| Model Name                                | Key Feature(s)                                                      |\n",
    "|-------------------------------------------|----------------------------------------------------------------------|\n",
    "| `meta-llama/Llama-3.2-1B`                 | Standard causal LM (Grouped Attention)                              |\n",
    "| `Qwen/Qwen2.5-1.5B`                        | RMSNorm architecture                                                |\n",
    "| `apple/OpenELM-1_1B`                       | QKV combined in a **single Linear layer**                           |\n",
    "| `mistralai/Mistral-7B-v0.3` (quantized)    | Sliding Window Attention                                            |\n",
    "| `allenai/OLMoE-1B-7B-0924` (quantized)     | Mixture of Experts                                                  |\n",
    "| `Dream-org/Dream-v0-Base-7B` (quantized)   | Diffusion-based model (non-causal, bidirectional)                   |\n",
    "\n",
    "---\n",
    "\n",
    "## Core Training Variants (All Models)\n",
    "\n",
    "For **each listed model**, train with:\n",
    "\n",
    "1. `delta_rule` â€“ constant `mag_weight = 0.5`  \n",
    "2. `delta_product` â€“ constant `mag_weight = 0.5`  \n",
    "\n",
    "---\n",
    "\n",
    "## LLaMA-Specific Variants\n",
    "\n",
    "For **`meta-llama/Llama-3.2-1B`** add:\n",
    "\n",
    "- Same as above **but without LoRA**:  \n",
    "  - `delta_rule` â€“ constant `mag_weight = 0.5`  \n",
    "  - `delta_product` â€“ constant `mag_weight = 0.5`  \n",
    "\n",
    "---\n",
    "\n",
    "## Operator Mode Experiments (*LLaMA only*, constant `mag_weight = 0.5`)\n",
    "\n",
    "- âœ… `delta_rule` (*already done*)  \n",
    "- `delta_rule + non-linearity`  \n",
    "- âœ… `delta_product` (*already done*)  \n",
    "- `delta_product + rotation`  \n",
    "- `delta_product_derived + rotation` (combined approach)  \n",
    "\n",
    "---\n",
    "\n",
    "## Liza Callback Sweeps (*LLaMA only*)\n",
    "\n",
    "Test various **mag_weight schedules** for both `delta_rule` and `delta_product`:\n",
    "\n",
    "1. **Constant values**: 0.125, 0.25, 0.5 (âœ…), 0.75  \n",
    "2. **Transition to 0.5** over: 10, 100, 1000 samples  \n",
    "3. **Alternating schedules**:\n",
    "    - [0, 0.5]  \n",
    "    - [0, 1.0]  \n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Setup\n",
    "\n",
    "- **Dataset:** 10% of **alpaca-cleaned** (~5000 samples)  \n",
    "- **Padding:** `\"longest\"`, max length **386**  \n",
    "- **Test set:** 2 sample examples per run (not critical â€” just sanity check)  \n",
    "- **Seed:** `42` (fixed across all runs)  \n",
    "\n",
    "---\n",
    "\n",
    "## Max Chunk Size Rules\n",
    "\n",
    "- `delta_rule`: **64**  \n",
    "- `delta_product`: **32** (due to doubled sequence length)  \n",
    "\n",
    "---\n",
    "\n",
    "## Training Parameters (Global Defaults)\n",
    "\n",
    "- Same LoRA setup, training config, and tokenizer unless otherwise stated  \n",
    "- Only reduce batch size when memory is insufficient  \n",
    "- Special handling: diffusion models have their own requirements  \n",
    "- Hardware Target: **2Ã— T4 GPUs (Kaggle free tier)** to demonstrate *low-compute viability*  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
