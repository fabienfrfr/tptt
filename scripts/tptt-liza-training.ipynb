{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install -q bitsandbytes accelerate\n#!pip install -q -U git+https://github.com/fabienfrfr/tptt@main\n!pip install -q -U git+https://github.com/fabienfrfr/tptt@dev","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:29:11.710388Z","iopub.execute_input":"2025-06-07T13:29:11.710731Z","iopub.status.idle":"2025-06-07T13:31:15.293152Z","shell.execute_reply.started":"2025-06-07T13:29:11.710705Z","shell.execute_reply":"2025-06-07T13:31:15.291840Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for tptt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# only in kaggle for HF\nfrom huggingface_hub import login, HfApi\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\napi = HfApi()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:31:15.296082Z","iopub.execute_input":"2025-06-07T13:31:15.296436Z","iopub.status.idle":"2025-06-07T13:31:16.202866Z","shell.execute_reply.started":"2025-06-07T13:31:15.296398Z","shell.execute_reply":"2025-06-07T13:31:16.201858Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"QUANTIZATION = False\n\nbase_model_name=\"meta-llama/Llama-3.2-1B\"\nbase_tokenizer_name=base_model_name\n\n# basic training\nN = 100\nEPOCH = 10\n\n# saving\nusername = \"ffurfaro\"\nmodel_name = '/Titans-' + base_model_name.split('/')[1]\ndir_path = '.' + model_name\nrepo_id = username + '/' + model_name.lstrip('/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:31:16.203879Z","iopub.execute_input":"2025-06-07T13:31:16.204232Z","iopub.status.idle":"2025-06-07T13:31:16.210050Z","shell.execute_reply.started":"2025-06-07T13:31:16.204201Z","shell.execute_reply":"2025-06-07T13:31:16.209063Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport shutil\nimport json\n\nif QUANTIZATION:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport torch\ntorch.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, \n    DataCollatorForLanguageModeling, TrainerCallback\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftConfig, get_peft_model\n\nif QUANTIZATION:\n    from transformers import BitsAndBytesConfig\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n    )\nelse :\n    bnb_config = None\n\n# Tools\nimport tptt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:31:16.212588Z","iopub.execute_input":"2025-06-07T13:31:16.212856Z","iopub.status.idle":"2025-06-07T13:31:51.140751Z","shell.execute_reply.started":"2025-06-07T13:31:16.212834Z","shell.execute_reply":"2025-06-07T13:31:51.139878Z"}},"outputs":[{"name":"stderr","text":"2025-06-07 13:31:32.408754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749303092.674963      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749303092.756628      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]  # Llama, Mistral, OLMo. Minimal : q_proj, v_proj\n#target_modules = [\"qkv_proj\",\"out_proj\"]  # OpenELM,\n#target_modules = [\"c_attn\",\"c_proj\"]  # GPT-2\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=target_modules,\n).to_dict()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:31:51.141690Z","iopub.execute_input":"2025-06-07T13:31:51.142296Z","iopub.status.idle":"2025-06-07T13:31:51.148032Z","shell.execute_reply.started":"2025-06-07T13:31:51.142272Z","shell.execute_reply":"2025-06-07T13:31:51.146956Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"## Transforming into Titans\nconfig = tptt.TpttConfig(\n    base_model_name=base_model_name,\n    max_self_attn_length = 2048,\n    lora_config=lora_config,\n)\n\nmodel = tptt.TpttModel(config,\n    trust_remote_code=True,\n    attn_implementation=\"eager\",\n    token=hf_token,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,)\n#display(model)\nmodel.backbone.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:31:51.149288Z","iopub.execute_input":"2025-06-07T13:31:51.149623Z","iopub.status.idle":"2025-06-07T13:32:07.526134Z","shell.execute_reply.started":"2025-06-07T13:31:51.149594Z","shell.execute_reply":"2025-06-07T13:32:07.525050Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9beaa858e35841c38b9816c204f66350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40fe6fc4e25400eac59885be0f16092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a50164e438a84a18b18eeb6750734047"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name, token=hf_token)\n# Ensure the tokenizer has a padding token for batching\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token or \"[PAD]\"\n\nraw_dataset = load_dataset(\"yahma/alpaca-cleaned\")[\"train\"].select(range(N))\n\ndef preprocess_fn(samples):\n    \"\"\"\n    Tokenize the samples for causal language modeling.\n    Concatenate instruction, input, and output as needed.\n    \"\"\"\n    prompts = [\n        f\"{instr}\\n{inp}\" if inp else instr\n        for instr, inp in zip(samples[\"instruction\"], samples[\"input\"])\n    ]\n    # Optionally, append output for supervised fine-tuning\n    prompts = [f\"{p}\\n{out}\" for p, out in zip(prompts, samples[\"output\"])]\n    tokens = tokenizer(\n        prompts,\n        truncation=True,\n        max_length=512, #256,\n        padding=\"longest\", #padding= \"max_length\",\n        return_attention_mask=True,\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ntokenized_dataset = raw_dataset.map(\n    preprocess_fn, batched=True, remove_columns=raw_dataset.column_names\n)\n\n# Tokenize the dataset in batches and remove original columns\ntokenized_dataset = raw_dataset.map(\n    preprocess_fn, batched=True, remove_columns=raw_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:32:07.527265Z","iopub.execute_input":"2025-06-07T13:32:07.527969Z","iopub.status.idle":"2025-06-07T13:32:13.608179Z","shell.execute_reply.started":"2025-06-07T13:32:07.527942Z","shell.execute_reply":"2025-06-07T13:32:13.607287Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5408ccbe5c7b485da6163963e2006569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b300fd49db6a422aaa86bda94a31b096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4a0c50987784f45ab41f8c4f90a8e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b193ccfe2c7495e8d1ff61c28af7a6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9710c2fe9a0498b81ed47f8e7396265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c24f0951b2074e0ba6b484558ff97f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae52e33c00c46dbab162df8c2cd78d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d51a4d0268485e96fbe7178255fac9"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\n# Step 6: Define HuggingFace TrainingArguments for reproducible training\ntraining_args = TrainingArguments(\n    output_dir=\"./tptt_output\",\n    per_device_train_batch_size=3, # per_device_train_batch_size * N GPU --> VRAM limit risk \n    num_train_epochs=EPOCH,\n    learning_rate=  5e-4,\n    max_grad_norm=1.0, # gradiant clipping\n    fp16=True,  # Use mixed precision if supported by hardware\n    ddp_find_unused_parameters=False, \n    logging_steps=5,\n    save_total_limit=2,  # Limit HDD\n    seed=42,\n    save_strategy=\"epoch\",\n    report_to=\"tensorboard\",\n)\n\n# LiZA MaG callback\ninitial_weight=0.01,\nfinal_weight=0.5,\ntransition_step=100,\nliza_callback = tptt.AdjustMaGWeightCallback(\n            model,\n            initial_weight=initial_weight,\n            final_weight=final_weight,\n            transition_step=transition_step,)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n    processing_class=tokenizer,\n    callbacks=[liza_callback],\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:37:37.239979Z","iopub.execute_input":"2025-06-07T13:37:37.241698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.save_pretrained(dir_path)\nmodel.save_pretrained(dir_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:32:13.609377Z","iopub.execute_input":"2025-06-07T13:32:13.609744Z","iopub.status.idle":"2025-06-07T13:32:20.720524Z","shell.execute_reply.started":"2025-06-07T13:32:13.609703Z","shell.execute_reply":"2025-06-07T13:32:20.719279Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"api.create_repo(\n    repo_id=repo_id,\n    token=hf_token,\n    repo_type=\"model\",\n    exist_ok=True,\n    private=False\n)\napi.upload_folder(\n    folder_path=dir_path,\n    repo_id=repo_id,\n    repo_type=\"model\",\n    token=hf_token,\n    commit_message=\"Upload model + init tptt code\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:32:20.722854Z","iopub.execute_input":"2025-06-07T13:32:20.723273Z","iopub.status.idle":"2025-06-07T13:33:15.300662Z","shell.execute_reply.started":"2025-06-07T13:32:20.723236Z","shell.execute_reply":"2025-06-07T13:33:15.299616Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb8dc6dc0f6482fbeb057a34c9065d5"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ffurfaro/Titans-Llama-3.2-1B/commit/eeb89fa3ed03f30589bc3aea5040189269d04633', commit_message='Upload model + init tptt code', commit_description='', oid='eeb89fa3ed03f30589bc3aea5040189269d04633', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ffurfaro/Titans-Llama-3.2-1B', endpoint='https://huggingface.co', repo_type='model', repo_id='ffurfaro/Titans-Llama-3.2-1B'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nmodel_tptt = AutoModelForCausalLM.from_pretrained(repo_id, token=hf_token, trust_remote_code=True)\ndisplay(model_tptt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T13:33:15.303102Z","iopub.execute_input":"2025-06-07T13:33:15.303401Z","iopub.status.idle":"2025-06-07T13:33:55.566067Z","shell.execute_reply.started":"2025-06-07T13:33:15.303378Z","shell.execute_reply":"2025-06-07T13:33:55.565103Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e135e66cae4aa7af29da5588c97003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_tptt.py:   0%|          | 0.00/3.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1000abcf27d3405fb393f6d86eb5199d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ffurfaro/Titans-Llama-3.2-1B:\n- configuration_tptt.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_tptt.py:   0%|          | 0.00/27.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af13a7fb487f4453ad5efffefe2dc190"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ffurfaro/Titans-Llama-3.2-1B:\n- modeling_tptt.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6235ff88892d4686b8680c870a2faeed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/6.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb2ddaf26b5a4f0cb480a45d59382ab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"TpttModel(\n  (backbone): PeftModelForCausalLM(\n    (base_model): LoraModel(\n      (model): LlamaForCausalLM(\n        (model): LlamaModel(\n          (embed_tokens): Embedding(128256, 2048)\n          (layers): ModuleList(\n            (0-15): 16 x LlamaDecoderLayer(\n              (self_attn): LiZAttention(\n                (base_attn): LlamaAttention(\n                  (q_proj): lora.Linear(\n                    (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2048, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=2048, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k_proj): lora.Linear(\n                    (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2048, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=512, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (v_proj): lora.Linear(\n                    (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2048, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=512, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o_proj): lora.Linear(\n                    (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2048, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=2048, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                )\n                (operator): AttentionOperator()\n                (pool_g): AdaptiveAvgPool1d(output_size=512)\n              )\n              (mlp): LlamaMLP(\n                (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n                (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n                (act_fn): SiLU()\n              )\n              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            )\n          )\n          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":10}]}