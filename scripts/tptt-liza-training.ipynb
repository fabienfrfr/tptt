{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes accelerate\n!pip install -q -U git+https://github.com/fabienfrfr/tptt@main","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# only in kaggle for HF\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"QUANTIZATION = False\n\nbase_model_name=\"meta-llama/Llama-3.2-1B\"\ntokenizer_name=base_model_name\n\n# basic training\nN = 100\nEPOCH = 10\n\n# saving\nusername = \"ffurfaro\"\nmodel_name = '/Titans-' + base_model_name.split('/')[1]\ndir_path = '.' + model_name\nrepo_id = username + '/' + model_name.lstrip('/')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport json\n\nif QUANTIZATION:\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nimport torch\ntorch.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nfrom huggingface_hub import HfApi\nfrom transformers import (\n    AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, \n    DataCollatorForLanguageModeling, TrainerCallback\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\n\nfrom transformers import BitsAndBytesConfig\n\nif QUANTIZATION:\n    from transformers import BitsAndBytesConfig\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n    )\nelse :\n    bnb_config = None\n\n# Tools\nimport tptt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Load backbone\nbackbone = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name,\n    trust_remote_code=True,\n    attn_implementation=\"eager\",\n    token=hf_token,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n)\n\n## LoRA adapter\nlora_candidates = [\n    \"q_proj\", ## minimal\n    \"k_proj\",\n    \"v_proj\", ## minimal\n    \"o_proj\",  # Llama, Mistral, OLMo\n    \"qkv_proj\",\n    \"out_proj\",  # OpenELM,\n    \"c_attn\",\n    \"c_proj\",  # GPT-2\n]\n\ntarget_modules = [\n    name\n    for name, _ in backbone.named_modules()\n    if any(name.endswith(n) for n in lora_candidates)\n]\ntarget_modules = list(set(target_modules))\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=target_modules,\n)\n# Inject LoRA adapters (external function, not shown here)\nbackbone = get_peft_model(backbone, lora_config)\nbackbone.print_trainable_parameters()\n\n## Transforming into Titans\nmodel = tptt.TpttModel(config, backbone=backbone)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_tokenizer_name, token=hf_token)\n# Ensure the tokenizer has a padding token for batching\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token or \"[PAD]\"\n\nraw_dataset = load_dataset(\"yahma/alpaca-cleaned\")[\"train\"].select(range(N))\n\ndef preprocess_fn(samples):\n    \"\"\"\n    Tokenize the samples for causal language modeling.\n    Concatenate instruction, input, and output as needed.\n    \"\"\"\n    prompts = [\n        f\"{instr}\\n{inp}\" if inp else instr\n        for instr, inp in zip(samples[\"instruction\"], samples[\"input\"])\n    ]\n    # Optionally, append output for supervised fine-tuning\n    prompts = [f\"{p}\\n{out}\" for p, out in zip(prompts, samples[\"output\"])]\n    tokens = tokenizer(\n        prompts,\n        truncation=True,\n        max_length=512, #256,\n        padding=\"longest\", #padding= \"max_length\",\n        return_attention_mask=True,\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ntokenized_dataset = raw_dataset.map(\n    preprocess_fn, batched=True, remove_columns=raw_dataset.column_names\n)\n\n# Tokenize the dataset in batches and remove original columns\ntokenized_dataset = raw_dataset.map(\n    preprocess_fn, batched=True, remove_columns=raw_dataset.column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False,\n)\n\n# Step 6: Define HuggingFace TrainingArguments for reproducible training\ntraining_args = TrainingArguments(\n    output_dir=\"./tptt_output\",\n    per_device_train_batch_size=4, # per_device_train_batch_size * N GPU --> VRAM limit risk \n    num_train_epochs=EPOCH,\n    learning_rate=  5e-4,\n    max_grad_norm=1.0, # gradiant clipping\n    fp16=True,  # Use mixed precision if supported by hardware\n    ddp_find_unused_parameters=False, \n    logging_steps=5,\n    save_total_limit=2,  # Limit HDD\n    seed=42,\n    save_strategy=\"epoch\",\n    report_to=\"tensorboard\",\n)\n\n# LiZA MaG callback\ninitial_weight=0.01,\nfinal_weight=0.5,\ntransition_step=100,\nliza_callback = tptt.AdjustMaGWeightCallback(\n            model,\n            initial_weight=initial_weight,\n            final_weight=final_weight,\n            transition_step=transition_step,)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n    processing_class=tokenizer,\n    callbacks=[liza_callback],\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(dir_path)\ntokenizer.save_pretrained(dir_path)\n\nconfig_path = os.path.join(dir_path, \"config.json\")\nwith open(config_path, \"r\") as f:\n    config = json.load(f)\nconfig[\"auto_map\"] = {\n  \"AutoModelForCausalLM\": \"modeling_tptt.TpttModel\",\n  \"AutoConfig\": \"configuration_tptt.TpttConfig\"\n}\nconfig[\"architectures\"] = [\"TpttModel\"]\nwith open(config_path, \"w\") as f:\n    json.dump(config, f, indent=2)\n\napi = HfApi()\napi.create_repo(\n    repo_id=repo_id,\n    token=hf_token,\n    repo_type=\"model\",\n    exist_ok=True,\n    private=False\n)\napi.upload_folder(\n    folder_path=dir_path,\n    repo_id=repo_id,\n    repo_type=\"model\",\n    token=hf_token,\n    commit_message=\"Upload init + code custom\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}