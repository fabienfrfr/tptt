{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U git+https://github.com/fabienfrfr/tptt@dev\n!pip install -q -U lighteval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:20:17.777431Z","iopub.execute_input":"2025-06-05T19:20:17.777729Z","iopub.status.idle":"2025-06-05T19:22:40.240685Z","shell.execute_reply.started":"2025-06-05T19:20:17.777706Z","shell.execute_reply":"2025-06-05T19:22:40.239136Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for tptt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.0/433.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntptt 0.1.0 requires transformers<=4.49.0,>=4.45.0, but you have transformers 4.52.4 which is incompatible.\nomegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\ngoogle-genai 1.9.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# only in kaggle for HF\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:22:40.243386Z","iopub.execute_input":"2025-06-05T19:22:40.243776Z","iopub.status.idle":"2025-06-05T19:22:40.355972Z","shell.execute_reply.started":"2025-06-05T19:22:40.243741Z","shell.execute_reply":"2025-06-05T19:22:40.355143Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"base_model_name=\"meta-llama/Llama-3.2-1B\"\ntokenizer_name=base_model_name\n\n# saving\nusername = \"ffurfaro\"\nmodel_name = '/Titans-' + base_model_name.split('/')[1]\nrepo_id = username + '/' + model_name.lstrip('/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:22:40.356906Z","iopub.execute_input":"2025-06-05T19:22:40.357152Z","iopub.status.idle":"2025-06-05T19:22:40.362832Z","shell.execute_reply.started":"2025-06-05T19:22:40.357132Z","shell.execute_reply":"2025-06-05T19:22:40.361678Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch, tptt\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tptt.modeling_tptt import TpttModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:22:40.364890Z","iopub.execute_input":"2025-06-05T19:22:40.365262Z","iopub.status.idle":"2025-06-05T19:23:14.678837Z","shell.execute_reply.started":"2025-06-05T19:22:40.365231Z","shell.execute_reply":"2025-06-05T19:23:14.677838Z"}},"outputs":[{"name":"stderr","text":"2025-06-05 19:22:57.364587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749151377.622119      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749151377.694239      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model_tptt = TpttModel.from_pretrained(repo_id, token=hf_token)\ntokenizer = AutoTokenizer.from_pretrained(repo_id, token=hf_token)\ntokenizer.padding_side = \"left\"\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:43:04.920755Z","iopub.execute_input":"2025-06-05T17:43:04.921406Z","iopub.status.idle":"2025-06-05T17:43:28.537112Z","shell.execute_reply.started":"2025-06-05T17:43:04.921376Z","shell.execute_reply":"2025-06-05T17:43:28.536114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/494 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59984fbade31456a95e94a795d008fc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bcc27d7534f4e6d931de86c368d0656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7695fe7c92f473c9f9d5a34c6165528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f790ae0a0048559e6e65bf187a8ab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d33407e8c9c4e9d8d11804ef9924def"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/6.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa8f5a143d5542d7a40114ed2983dae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8872b18639f84ba69cd722789168f54b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7dbe3fcd29e46298bb24dd3a2eede6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d6ccfdf24b4418a7319104b578b500"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"device = 0 if torch.cuda.is_available() else -1\nmodel_tptt.to(f\"cuda:{device}\" if device != -1 else \"cpu\")\nmodel_tptt.eval()\n\npipe = tptt.TpttPipeline(model=model_tptt.backbone, tokenizer=tokenizer, device=None)\n\nresult = pipe(\"Bonjour, I'm Fabien Furfaro, \", max_new_tokens=512, do_sample=True, eos_token_id=None)\nprint(result[0][\"generated_text\"],'\\n', len(result[0][\"generated_text\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T17:43:28.538280Z","iopub.execute_input":"2025-06-05T17:43:28.538694Z","iopub.status.idle":"2025-06-05T17:43:45.616384Z","shell.execute_reply.started":"2025-06-05T17:43:28.538634Z","shell.execute_reply":"2025-06-05T17:43:45.615384Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Bonjour, I'm Fabien Furfaro, 23 years old and living in Paris. I speak French, English and Spanish, and I love to travel, read, watch movies and listen to music. I'm looking for a partner to share my adventures with, and I'm always open to new \n 243\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"eva_tptt_code = \"\"\"\nimport os\nimport torch\nfrom lighteval.models.abstract_model import LightevalModel\nfrom transformers import AutoTokenizer\nfrom tptt.modeling_tptt import TpttModel\n\nclass ModelInfo:\n    def __init__(self, model_name, model_sha=None, model_dtype=None, model_size=None):\n        self.model_name = model_name\n        self.model_sha = model_sha or \"\"\n        self.model_dtype = model_dtype or \"float32\"\n        self.model_size = model_size or \"unknown\"\n\nclass EvaTpttModel(LightevalModel):\n    def __init__(self, config):\n        super().__init__()\n        repo_id = os.environ.get(\"REPO_ID\")\n        hf_token = os.environ.get(\"HF_TOKEN\")\n        device = os.environ.get(\"DEVICE\", \"cpu\")\n        self.model = TpttModel.from_pretrained(repo_id, token=hf_token).to(device)\n        self._tokenizer = AutoTokenizer.from_pretrained(repo_id, token=hf_token)\n        self.device = device\n        self.model.eval()\n        try:\n            n_params = sum(p.numel() for p in self.model.parameters())\n        except Exception:\n            n_params = \"unknown\"\n        self.model_info = ModelInfo(\n            model_name=\"EvaTpttModel\",\n            model_sha=\"custom\",\n            model_dtype=str(self.model.dtype) if hasattr(self.model, \"dtype\") else \"float32\",\n            model_size=n_params\n        )\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def max_length(self):\n        return getattr(self._tokenizer, \"model_max_length\", 2048)\n\n    def add_special_tokens(self, tokens_dict):\n        return self._tokenizer.add_special_tokens(tokens_dict)\n\n    def greedy_until(self, requests, max_tokens=None, stop_sequences=None):\n        results = []\n        for req in requests:\n            prompt = getattr(req, \"prompt\", getattr(req, \"context\", \"\"))\n            inputs = self._tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n            with torch.no_grad():\n                output_ids = self.model.generate(\n                    **inputs,\n                    max_new_tokens=max_tokens or 32,\n                    eos_token_id=self._tokenizer.eos_token_id,\n                    pad_token_id=self._tokenizer.pad_token_id,\n                )\n            text = self._tokenizer.decode(output_ids[0], skip_special_tokens=True)\n            if stop_sequences:\n                for stop in stop_sequences:\n                    idx = text.find(stop)\n                    if idx != -1:\n                        text = text[:idx]\n                        break\n            results.append((text, None))\n        return results\n\n    def loglikelihood(self, requests, log=True):\n        results = []\n        for req in requests:\n            if hasattr(req, \"context\") and hasattr(req, \"continuation\"):\n                full_text = req.context + (req.continuation or \"\")\n            elif hasattr(req, \"prompt\"):\n                full_text = req.prompt\n            else:\n                raise ValueError(f\"Requête inattendue : {req}\")\n            inputs = self._tokenizer(full_text, return_tensors=\"pt\").to(self.device)\n            labels = inputs[\"input_ids\"].clone()\n            with torch.no_grad():\n                outputs = self.model(**inputs, labels=labels)\n                log_likelihood = -outputs.loss.item() * labels.size(1)\n            results.append((log_likelihood, True))\n        return results\n\n\n    def loglikelihood_rolling(self, requests):\n        results = []\n        for req in requests:\n            text = getattr(req, \"text\", getattr(req, \"prompt\", \"\"))\n            tokens = self._tokenizer.encode(text)\n            stride = 512\n            ll = 0.0\n            for i in range(0, len(tokens), stride):\n                input_ids = torch.tensor([tokens[i:i+stride]]).to(self.device)\n                with torch.no_grad():\n                    outputs = self.model(input_ids=input_ids, labels=input_ids)\n                    ll += -outputs.loss.item() * input_ids.size(1)\n            results.append((ll, True))\n        return results\n\n    def loglikelihood_single_token(self, requests):\n        results = []\n        for req in requests:\n            context = getattr(req, \"context\", \"\")\n            token = getattr(req, \"token\", \"\")\n            context_ids = self._tokenizer.encode(context, return_tensors=\"pt\").to(self.device)\n            token_id = self._tokenizer.encode(token, add_special_tokens=False)[0]\n            input_ids = torch.cat([context_ids, torch.tensor([[token_id]]).to(self.device)], dim=1)\n            labels = input_ids.clone()\n            with torch.no_grad():\n                outputs = self.model(input_ids=input_ids, labels=labels)\n                ll = -outputs.loss.item()\n            results.append((ll, True))\n        return results\n\"\"\"\n\nwith open(\"eva_tptt_model.py\", \"w\") as f:\n    f.write(eva_tptt_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T19:31:18.243918Z","iopub.execute_input":"2025-06-05T19:31:18.244305Z","iopub.status.idle":"2025-06-05T19:31:18.253288Z","shell.execute_reply.started":"2025-06-05T19:31:18.244282Z","shell.execute_reply":"2025-06-05T19:31:18.252288Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport torch\nfrom lighteval.pipeline import Pipeline, PipelineParameters, ParallelismManager\nfrom lighteval.logging.evaluation_tracker import EvaluationTracker\nfrom lighteval.models.custom.custom_model import CustomModelConfig\n\nos.environ[\"REPO_ID\"] = repo_id\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"DEVICE\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nCUSTOM_MODEL_PATH = \"eva_tptt_model.py\"\n\n\nevaluation_tracker = EvaluationTracker(\n    output_dir=\"./results\",\n    save_details=True\n)\n\npipeline_params = PipelineParameters(\n    launcher_type=ParallelismManager.ACCELERATE,\n    max_samples=5\n)\n\nmodel_config = CustomModelConfig(\n    model_name=\"eva-tptt-model\",\n    model_definition_file_path=CUSTOM_MODEL_PATH,\n)\n\ntasks=\"leaderboard|truthfulqa:mc|0|0\"\n\npipeline = Pipeline(\n    tasks=tasks,\n    pipeline_parameters=pipeline_params,\n    evaluation_tracker=evaluation_tracker,\n    model_config=model_config\n)\n\npipeline.evaluate()\npipeline.save_and_push_results()\npipeline.show_results()","metadata":{"execution":{"iopub.status.busy":"2025-06-05T19:31:20.320608Z","iopub.execute_input":"2025-06-05T19:31:20.320954Z","iopub.status.idle":"2025-06-05T19:31:32.537122Z","shell.execute_reply.started":"2025-06-05T19:31:20.320927Z","shell.execute_reply":"2025-06-05T19:31:32.535631Z"},"trusted":true},"outputs":[],"execution_count":null}]}