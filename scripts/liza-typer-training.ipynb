{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca57e5d-dc3b-4d7c-ba59-2a974eef40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import psutil\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def extract_model_size(base_model: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract the model size in number of parameters from a base_model string.\n",
    "    The function looks for the last number before 'B' and interprets it as billions of parameters.\n",
    "    \"\"\"\n",
    "    if \"B\" not in base_model:\n",
    "        raise ValueError(f\"Could not find 'B' in: {base_model}\")\n",
    "    \n",
    "    before_b = base_model.rsplit(\"B\", 1)[0]\n",
    "    \n",
    "    parts = re.split(r'[^0-9._]', before_b)\n",
    "    numbers = [p for p in parts if p.strip()]\n",
    "    last_num_str = numbers[-1].replace(\"_\", \".\").replace(\"-\", \".\")\n",
    "    \n",
    "    num_val = float(last_num_str)\n",
    "    return int(num_val * 1_000_000_000)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # Model parameters\n",
    "    base_model: str = \"meta-llama/Llama-3.2-1B\"\n",
    "    model_type: str = field(init=False)\n",
    "    batch_size: int = 2\n",
    "    quantization: bool = False\n",
    "    model_size: Optional[int] = None\n",
    "    \n",
    "    # Hardware\n",
    "    device: Optional[str] = None  # e.g., \"cuda:0\"\n",
    "\n",
    "    # General\n",
    "    operator: str = \"delta_rule\"\n",
    "    bidirectional: bool = False\n",
    "    max_chunk_size: int = 64\n",
    "    linear_precision: str = \"bfloat16\"\n",
    "    base_attention_scaling: bool = False\n",
    "\n",
    "    # MAG parameters\n",
    "    n_transition_steps: int = 10\n",
    "    initial_mag: float = 0.0\n",
    "    mag_ratio: float = 0.5\n",
    "    base_scale: bool = False\n",
    "    mag_mode: str = \"gradual\"\n",
    "    cross_gate: bool = False\n",
    "\n",
    "    # LoRA parameters\n",
    "    lora: bool = True\n",
    "    lora_rank: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "\n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 5e-4\n",
    "    weight_decay: float = 0.0\n",
    "    label_smooth: float = 0.0\n",
    "    accumulation_grad_step: int = 1\n",
    "    epoch: int = 1\n",
    "\n",
    "    # Dataset parameters\n",
    "    n_samples: int = 100\n",
    "    test_eval_percent: int = 2\n",
    "    dataset: str = \"yahma/alpaca-cleaned\"\n",
    "    max_seq_len: int = 256\n",
    "\n",
    "    # Hugging Face Hub parameters\n",
    "    hf_token: Optional[str] = None\n",
    "    username: str = \"ffurfaro\"\n",
    "    repo_name: str = \"Titans-v2-model\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Automatically set model_type based on base_model string\n",
    "        if self.model_type is None :\n",
    "            if \"mistral\" in self.base_model.lower():\n",
    "                self.model_type = \"mistral\"\n",
    "            else:\n",
    "                self.model_type = \"classic\"\n",
    "\n",
    "        self.model_size = extract_model_size(self.base_model) if self.model_size is None else self.model_size\n",
    "\n",
    "\n",
    "\n",
    "MODEL_TYPE = [\"classic\", \"moe\", \"mistral\"]\n",
    "\n",
    "DTYPE_SIZE = {\n",
    "    torch.float32: 4,\n",
    "    torch.float16: 2,\n",
    "    torch.bfloat16: 2,\n",
    "}\n",
    "\n",
    "def estimate_memory_mistral(batch_size: int, model_size: int, quantization: bool):\n",
    "    \"\"\"\n",
    "    Estimates GPU memory (GB) needed for Mistral fine-tuning (16-bit),\n",
    "    Slidding Windows Attention (SWA) change memory calculation\n",
    "    \"\"\"\n",
    "    # Reference base fine-tuning memory for Mistral 7B\n",
    "    base_ft_mem_gb = 52.0  # average between 50 and 55 GB\n",
    "\n",
    "    # Activation + optimizer overhead per batch (estimated higher than inference)\n",
    "    activation_per_batch_gb = 7.0  # rough estimate for fine-tuning\n",
    "\n",
    "    # Scale base memory by model size proportionally (default 7B)\n",
    "    base_mem_scaled = base_ft_mem_gb * (model_size / 7_000_000_000)\n",
    "\n",
    "    # Total memory estimate\n",
    "    total_mem = base_mem_scaled + batch_size * activation_per_batch_gb\n",
    "\n",
    "    # Quantization reduces memory roughly by factor 4\n",
    "    if quantization:\n",
    "        total_mem *= 0.25\n",
    "\n",
    "    return total_mem\n",
    "\n",
    "def estimate_memory_usage(cfg: TrainConfig):\n",
    "    \"\"\"\n",
    "    Estimate GPU memory (GB) needed for training given config, \n",
    "    compares to device availability and suggests adjustments if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    model_size = cfg.model_size\n",
    "    quantization = cfg.quantization\n",
    "    batch_size = cfg.batch_size\n",
    "    seq_len = cfg.max_seq_len\n",
    "\n",
    "    if cfg.model_type == \"mistral\":\n",
    "        total_mem_gb = estimate_memory_mistral(batch_size, model_size, quantization)\n",
    "    else:\n",
    "        # Generic estimation for other models based on params, dtype, batch, seq_len\n",
    "        dtype = torch.bfloat16 if not quantization else torch.float16\n",
    "        param_mem_bytes = model_size * DTYPE_SIZE[dtype]\n",
    "\n",
    "        activation_mem_bytes = batch_size * seq_len * 4 * DTYPE_SIZE[dtype] * 20  # rough estimate\n",
    "\n",
    "        total_mem_bytes = param_mem_bytes + activation_mem_bytes\n",
    "\n",
    "        # Extra 20% overhead for optimizer states, LoRA, etc.\n",
    "        optimizer_overhead = param_mem_bytes * 1.2\n",
    "        total_mem_bytes += optimizer_overhead\n",
    "\n",
    "        total_mem_gb = total_mem_bytes / (1024 ** 3)\n",
    "    \n",
    "    if cfg.bidirectional:\n",
    "        # Bidirectional models typically double the memory usage\n",
    "        total_mem_gb *= 2\n",
    "    if \"delta_product\" in cfg.operator:\n",
    "        # Delta product operator may increase memory usage\n",
    "        total_mem_gb *= 2\n",
    "\n",
    "    # Detect available memory on device\n",
    "    if torch.cuda.is_available() and \"cuda\" in cfg.device:\n",
    "        try:\n",
    "            gpu_index = int(cfg.device.split(\":\")[1])\n",
    "        except Exception:\n",
    "            gpu_index = 0\n",
    "        props = torch.cuda.get_device_properties(gpu_index)\n",
    "        available_mem_gb = props.total_memory / (1024 ** 3)\n",
    "        device_name = props.name\n",
    "    else:\n",
    "        mem = psutil.virtual_memory()\n",
    "        available_mem_gb = mem.available / (1024 ** 3)\n",
    "        device_name = \"CPU\"\n",
    "\n",
    "\n",
    "    print(f\"ðŸ” Memory estimation for {cfg.model_type} (batch size {batch_size})\")\n",
    "    print(f\"   Quantization    : {quantization}\")\n",
    "    print(f\"   Device          : {device_name}\")\n",
    "    print(f\"   Required memory : {total_mem_gb:.2f} GB\")\n",
    "    print(f\"   Available memory: {available_mem_gb:.2f} GB\")\n",
    "\n",
    "    suggestions = []\n",
    "    if total_mem_gb > available_mem_gb:\n",
    "        suggestions.append(\"Reduce batch size (by half or more)\")\n",
    "        suggestions.append(\"Enable quantization (--quantization True)\")\n",
    "        suggestions.append(\"Use a smaller model\")\n",
    "        suggestions.append(\"Use a different hardware (GPU with more memory)\")\n",
    "\n",
    "    if suggestions:\n",
    "        print(\"\\nâš ï¸ Not enough memory. Suggestions:\")\n",
    "        for s in suggestions:\n",
    "            print(f\"  - {s}\")\n",
    "    else:\n",
    "        print(\"âœ… Memory configuration OK.\")\n",
    "\n",
    "    return total_mem_gb, available_mem_gb, suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5528cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000\n",
      "1300000000\n",
      "7100000000\n",
      "13000000000\n"
     ]
    }
   ],
   "source": [
    "def extract_model_size(base_model: str) -> int:\n",
    "    \"\"\"\n",
    "    Extract the model size in number of parameters from a base_model string.\n",
    "    The function looks for the last number before 'B' and interprets it as billions of parameters.\n",
    "    \"\"\"\n",
    "    if \"B\" not in base_model:\n",
    "        raise ValueError(f\"Could not find 'B' in: {base_model}\")\n",
    "    \n",
    "    before_b = base_model.rsplit(\"B\", 1)[0]\n",
    "    \n",
    "    parts = re.split(r'[^0-9._]', before_b)\n",
    "    numbers = [p for p in parts if p.strip()]\n",
    "    last_num_str = numbers[-1].replace(\"_\", \".\").replace(\"-\", \".\")\n",
    "    \n",
    "    num_val = float(last_num_str)\n",
    "    return int(num_val * 1_000_000_000)\n",
    "\n",
    "\n",
    "# Tests\n",
    "print(extract_model_size(\"meta-llama/Llama-3.2-1B\"))  # 1_000_000_000\n",
    "print(extract_model_size(\"meta-llama/Llama-1_3B\"))    # 1_300_000_000\n",
    "print(extract_model_size(\"mistralai/Mistral-7.1B\"))     # 7_000_000_000\n",
    "print(extract_model_size(\"something3B-13B\"))           # 13_000_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dfa31cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     estimate_memory_usage(cfg)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mcheck_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcheck_memory\u001b[39m\u001b[34m(model_type, model_size, batch_size, quantization, device)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;129m@app\u001b[39m.command()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_memory\u001b[39m(\n\u001b[32m      7\u001b[39m     model_type: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mmistral\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     device: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m#\"cuda:0\",\u001b[39;00m\n\u001b[32m     12\u001b[39m ):\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m    Check GPU memory needed for fine-tuning a given model and suggest adjustments.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     cfg = \u001b[43mTrainConfig\u001b[49m(\n\u001b[32m     17\u001b[39m         model_type=model_type,\n\u001b[32m     18\u001b[39m         batch_size=batch_size,\n\u001b[32m     19\u001b[39m         quantization=quantization,\n\u001b[32m     20\u001b[39m         device=device,\n\u001b[32m     21\u001b[39m         model_size=model_size\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m     estimate_memory_usage(cfg)\n",
      "\u001b[31mNameError\u001b[39m: name 'TrainConfig' is not defined"
     ]
    }
   ],
   "source": [
    "import typer\n",
    "\n",
    "app = typer.Typer(add_completion=False)\n",
    "\n",
    "@app.command()\n",
    "def check_memory(\n",
    "    model_type: str = \"mistral\",\n",
    "    model_size: int = 7_000_000_000,\n",
    "    batch_size: int = 2,\n",
    "    quantization: bool = True,\n",
    "    device: str = \"cpu\", #\"cuda:0\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Check GPU memory needed for fine-tuning a given model and suggest adjustments.\n",
    "    \"\"\"\n",
    "    cfg = TrainConfig(\n",
    "        model_type=model_type,\n",
    "        batch_size=batch_size,\n",
    "        quantization=quantization,\n",
    "        device=device,\n",
    "        model_size=model_size\n",
    "    )\n",
    "    estimate_memory_usage(cfg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
