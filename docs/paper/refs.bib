@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{mann2020language,
  title   = {Language models are few-shot learners},
  author  = {Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others},
  journal = {arXiv preprint arXiv:2005.14165},
  volume  = {1},
  pages   = {3},
  year    = {2020}
}

@inproceedings{katharopoulos2020transformers,
  title        = {Transformers are rnns: Fast autoregressive transformers with linear attention},
  author       = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle    = {International conference on machine learning},
  pages        = {5156--5165},
  year         = {2020},
  organization = {PMLR}
}

@article{zhang2024lolcats,
  title   = {LoLCATs: On Low-Rank Linearizing of Large Language Models},
  author  = {Zhang, Michael and Arora, Simran and Chalamala, Rahul and Wu, Alan and Spector, Benjamin and Singhal, Aaryan and Ramesh, Krithik and R{\'e}, Christopher},
  journal = {arXiv preprint arXiv:2410.10254},
  year    = {2024}
}

@article{hu2022lora,
  title   = {Lora: Low-rank adaptation of large language models.},
  author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal = {ICLR},
  volume  = {1},
  number  = {2},
  pages   = {3},
  year    = {2022}
}


@article{lan2025liger,
  title   = {Liger: Linearizing Large Language Models to Gated Recurrent Structures},
  author  = {Lan, Disen and Sun, Weigao and Hu, Jiaxi and Du, Jusen and Cheng, Yu},
  journal = {arXiv preprint arXiv:2503.01496},
  year    = {2025}
}

@article{yang2024parallelizing,
  title   = {Parallelizing linear transformers with the delta rule over sequence length},
  author  = {Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  journal = {arXiv preprint arXiv:2406.06484},
  year    = {2024}
}


@article{behrouz2024titans,
  title   = {Titans: Learning to memorize at test time},
  author  = {Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  journal = {arXiv preprint arXiv:2501.00663},
  year    = {2024}
}

@article{dao2023flashattention,
  title   = {Flashattention-2: Faster attention with better parallelism and work partitioning},
  author  = {Dao, Tri},
  journal = {arXiv preprint arXiv:2307.08691},
  year    = {2023}
}

@article{gu2023mamba,
  title   = {Mamba: Linear-time sequence modeling with selective state spaces},
  author  = {Gu, Albert and Dao, Tri},
  journal = {arXiv preprint arXiv:2312.00752},
  year    = {2023}
}


@article{hendrycks2020measuring,
  title   = {Measuring massive multitask language understanding},
  author  = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2009.03300},
  year    = {2020}
}

@misc{lora_hf,
  author       = {Hugging Face},
  title        = {LoRA - Hugging Face PEFT documentation},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/docs/peft/main/conceptual_guides/lora}}
}


@article{wang2020linformer,
  title   = {Linformer: Self-attention with linear complexity},
  author  = {Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal = {arXiv preprint arXiv:2006.04768},
  year    = {2020}
}

@article{touvron2023llama,
  title   = {Llama: Open and efficient foundation language models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}

@article{mehta2024openelm,
  title   = {Openelm: An efficient language model family with open-source training and inference framework},
  author  = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and others},
  journal = {arXiv e-prints},
  pages   = {arXiv--2404},
  year    = {2024}
}

@article{bai2023qwen,
  title   = {Qwen technical report},
  author  = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal = {arXiv preprint arXiv:2309.16609},
  year    = {2023}
}

@article{groeneveld2024olmo,
  title   = {Olmo: Accelerating the science of language models},
  author  = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  journal = {arXiv preprint arXiv:2402.00838},
  year    = {2024}
}

@article{taori2023alpaca,
  title   = {Alpaca: A strong, replicable instruction-following model},
  author  = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal = {Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume  = {3},
  number  = {6},
  pages   = {7},
  year    = {2023}
}

@article{mercat2024linearizing,
  title   = {Linearizing large language models},
  author  = {Mercat, Jean and Vasiljevic, Igor and Keh, Sedrick and Arora, Kushal and Dave, Achal and Gaidon, Adrien and Kollar, Thomas},
  journal = {arXiv preprint arXiv:2405.06640},
  year    = {2024}
}

@article{munkhdalai2024leave,
  title   = {Leave no context behind: Efficient infinite context transformers with infini-attention},
  author  = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal = {arXiv preprint arXiv:2404.07143},
  volume  = {101},
  year    = {2024}
}


%% TODO: Add more references as needed

@article{chen2024computational,
  title   = {The computational limits of state-space models and mamba via the lens of circuit complexity},
  author  = {Chen, Yifang and Li, Xiaoyu and Liang, Yingyu and Shi, Zhenmei and Song, Zhao},
  journal = {arXiv preprint arXiv:2412.06148},
  year    = {2024}
}

@misc{jiang2023mistral7b,
  title   = {Mistral 7B},
  author  = {Albert Q. Jiang and all Mistral team},
  year    = {2023},
  journal = {arXiv preprint arXiv:2310.06825}
}

@article{grazzi2024unlocking,
  title   = {Unlocking state-tracking in linear rnns through negative eigenvalues},
  author  = {Grazzi, Riccardo and Siems, Julien and Zela, Arber and Franke, J{\"o}rg KH and Hutter, Frank and Pontil, Massimiliano},
  journal = {arXiv preprint arXiv:2411.12537},
  year    = {2024}
}