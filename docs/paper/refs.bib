@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{mann2020language,
  title   = {Language models are few-shot learners},
  author  = {Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others},
  journal = {arXiv preprint arXiv:2005.14165},
  volume  = {1},
  pages   = {3},
  year    = {2020}
}

@inproceedings{katharopoulos2020transformers,
  title        = {Transformers are rnns: Fast autoregressive transformers with linear attention},
  author       = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle    = {International conference on machine learning},
  pages        = {5156--5165},
  year         = {2020},
  organization = {PMLR}
}

@article{dao2024transformers,
  title   = {Transformers are ssms: Generalized models and efficient algorithms through structured state space duality},
  author  = {Dao, Tri and Gu, Albert},
  journal = {arXiv preprint arXiv:2405.21060},
  year    = {2024}
}


@article{zhang2024lolcats,
  title   = {LoLCATs: On Low-Rank Linearizing of Large Language Models},
  author  = {Zhang, Michael and Arora, Simran and Chalamala, Rahul and Wu, Alan and Spector, Benjamin and Singhal, Aaryan and Ramesh, Krithik and R{\'e}, Christopher},
  journal = {arXiv preprint arXiv:2410.10254},
  year    = {2024}
}

@article{hu2022lora,
  title   = {Lora: Low-rank adaptation of large language models.},
  author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal = {ICLR},
  volume  = {1},
  number  = {2},
  pages   = {3},
  year    = {2022}
}


@article{lan2025liger,
  title   = {Liger: Linearizing Large Language Models to Gated Recurrent Structures},
  author  = {Lan, Disen and Sun, Weigao and Hu, Jiaxi and Du, Jusen and Cheng, Yu},
  journal = {arXiv preprint arXiv:2503.01496},
  year    = {2025}
}

@article{yang2024parallelizing,
  title   = {Parallelizing linear transformers with the delta rule over sequence length},
  author  = {Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  journal = {arXiv preprint arXiv:2406.06484},
  year    = {2024}
}


@article{behrouz2024titans,
  title   = {Titans: Learning to memorize at test time},
  author  = {Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  journal = {arXiv preprint arXiv:2501.00663},
  year    = {2024}
}

@article{dao2023flashattention,
  title   = {Flashattention-2: Faster attention with better parallelism and work partitioning},
  author  = {Dao, Tri},
  journal = {arXiv preprint arXiv:2307.08691},
  year    = {2023}
}

@article{gu2023mamba,
  title   = {Mamba: Linear-time sequence modeling with selective state spaces},
  author  = {Gu, Albert and Dao, Tri},
  journal = {arXiv preprint arXiv:2312.00752},
  year    = {2023}
}


@article{hendrycks2020measuring,
  title   = {Measuring massive multitask language understanding},
  author  = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2009.03300},
  year    = {2020}
}

@misc{lora_hf,
  author       = {Hugging Face},
  title        = {LoRA - Hugging Face PEFT documentation},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/docs/peft/main/conceptual_guides/lora}}
}


@article{wang2020linformer,
  title   = {Linformer: Self-attention with linear complexity},
  author  = {Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal = {arXiv preprint arXiv:2006.04768},
  year    = {2020}
}

@article{touvron2023llama,
  title   = {Llama: Open and efficient foundation language models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}

@article{mehta2024openelm,
  title   = {Openelm: An efficient language model family with open-source training and inference framework},
  author  = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and others},
  journal = {arXiv e-prints},
  pages   = {arXiv--2404},
  year    = {2024}
}

@article{bai2023qwen,
  title   = {Qwen technical report},
  author  = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal = {arXiv preprint arXiv:2309.16609},
  year    = {2023}
}

@article{groeneveld2024olmo,
  title   = {Olmo: Accelerating the science of language models},
  author  = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  journal = {arXiv preprint arXiv:2402.00838},
  year    = {2024}
}

@article{taori2023alpaca,
  title   = {Alpaca: A strong, replicable instruction-following model},
  author  = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal = {Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume  = {3},
  number  = {6},
  pages   = {7},
  year    = {2023}
}

@article{mercat2024linearizing,
  title   = {Linearizing large language models},
  author  = {Mercat, Jean and Vasiljevic, Igor and Keh, Sedrick and Arora, Kushal and Dave, Achal and Gaidon, Adrien and Kollar, Thomas},
  journal = {arXiv preprint arXiv:2405.06640},
  year    = {2024}
}

@article{munkhdalai2024leave,
  title   = {Leave no context behind: Efficient infinite context transformers with infini-attention},
  author  = {Munkhdalai, Tsendsuren and Faruqui, Manaal and Gopal, Siddharth},
  journal = {arXiv preprint arXiv:2404.07143},
  volume  = {101},
  year    = {2024}
}


@article{rumelhart1986learning,
  title     = {Learning representations by back-propagating errors},
  author    = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal   = {nature},
  volume    = {323},
  number    = {6088},
  pages     = {533--536},
  year      = {1986},
  publisher = {Nature Publishing Group UK London}
}

@article{siems2025deltaproduct,
  title   = {Deltaproduct: Improving state-tracking in linear rnns via householder products},
  author  = {Siems, Julien and Carstensen, Timur and Zela, Arber and Hutter, Frank and Pontil, Massimiliano and Grazzi, Riccardo},
  journal = {arXiv preprint arXiv:2502.10297},
  year    = {2025}
}

@article{merrill2024illusion,
  title   = {The illusion of state in state-space models},
  author  = {Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  journal = {arXiv preprint arXiv:2404.08819},
  year    = {2024}
}


@misc{jiang2023mistral7b,
  title   = {Mistral 7B},
  author  = {Albert Q. Jiang and all Mistral team},
  year    = {2023},
  journal = {arXiv preprint arXiv:2310.06825}
}

@article{muennighoff2024olmoe,
  title   = {Olmoe: Open mixture-of-experts language models},
  author  = {Muennighoff, Niklas and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Morrison, Jacob and Min, Sewon and Shi, Weijia and Walsh, Pete and Tafjord, Oyvind and Lambert, Nathan and others},
  journal = {arXiv preprint arXiv:2409.02060},
  year    = {2024}
}

@article{team2025gemma,
  title   = {Gemma 3 technical report},
  author  = {Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal = {arXiv preprint arXiv:2503.19786},
  year    = {2025}
}

@article{behrouz2025s,
  title   = {It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization},
  author  = {Behrouz, Ali and Razaviyayn, Meisam and Zhong, Peilin and Mirrokni, Vahab},
  journal = {arXiv preprint arXiv:2504.13173},
  year    = {2025}
}

@article{dosovitskiy2020image,
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal = {arXiv preprint arXiv:2010.11929},
  year    = {2020}
}

@inproceedings{devlin2019bert,
  title     = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages     = {4171--4186},
  year      = {2019}
}

@article{ye2025dream,
  title   = {Dream 7B: Diffusion Large Language Models},
  author  = {Ye, Jiacheng and Xie, Zhihui and Zheng, Lin and Gao, Jiahui and Wu, Zirui and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng},
  journal = {arXiv preprint arXiv:2508.15487},
  year    = {2025}
}

@article{choromanski2020rethinking,
  title   = {Rethinking attention with performers},
  author  = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal = {arXiv preprint arXiv:2009.14794},
  year    = {2020}
}

@article{peng2023rwkv,
  title   = {Rwkv: Reinventing rnns for the transformer era},
  author  = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others},
  journal = {arXiv preprint arXiv:2305.13048},
  year    = {2023}
}

@article{shao2024deepseekmath,
  title   = {Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author  = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Yang and others},
  journal = {arXiv preprint arXiv:2402.03300},
  year    = {2024}
}