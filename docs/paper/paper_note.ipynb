{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8edfca-3b03-4d8f-8e37-44776fcd085b",
   "metadata": {},
   "source": [
    "# TPTT : Transforming Pretrained Transformer into Titans\n",
    "\n",
    "## Abstract\n",
    "In this paper, we introduce a novel approach to enhance the capabilities of pretrained Transformer models by integrating efficient linearized attention mechanisms: the TPTT framework. Our method, referred to as Transforming Pretrained Transformer into Titans (TPTT), leverages advanced techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA) to improve the performance and scalability of large language models. TPTT is integrated with the Transformers library for easy model adaptation, training, and inference, and can transform all Causal LLMs into Titans. Inspired by the Titans paper, we demonstrate the effectiveness of our approach through extensive experiments on MMLU benchmarks, demonstrating significant improvements in both efficiency and accuracy. For instance, our Titans-Llama-3.2-1B model demonstrates a 20% improvement in Exact Match (EM) compared to the baseline model. The source code is available on [GitHub](https://github.com/fabienfrfr/tptt).\n",
    "\n",
    "## Introduction\n",
    "The advent of large language models (LLMs) has revolutionized the field of natural language processing (NLP). However, the computational complexity and memory requirements of these models pose significant challenges, particularly during inference. To address these issues, many approach exist, and with different strategy, training from scratch (flash-linear-attention library, mamba) or convert softmax to linear attention (LoLCat, Liger), Liger can also exploit the same property defined here, but not by injection of linear. Here, we propose TPTT, a method that transforms pretrained Transformer models into more efficient and scalable architectures. By incorporating linearized attention mechanisms and advanced memory management techniques, TPTT aims to reduce the computational overhead while maintaining high performance. The goal is to exploit already trained models to easily transform them into models with memory and that only with fine tuning with LoRA.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Linearized Attention Mechanisms\n",
    "Linearized attention mechanisms are designed to reduce the computational complexity of traditional attention mechanisms. By approximating the softmax attention with linear operations, these mechanisms enable efficient processing of long sequences without compromising performance.\n",
    "\n",
    "### Memory as Gate (MaG)\n",
    "The Memory as Gate (MaG) technique is inspired by the concept of using memory to control the flow of information within the model. Decribes in Titans paper, this approach allows the model to retain important information over long sequences, thereby enhancing its ability to capture long-term dependencies. Because you can create same output with linear and softmax, the idea is simply to combine the mean of this to output for create memory mecanisms.\n",
    "\n",
    "### Integration with Pretrained Models\n",
    "Our method involves injecting linearized attention mechanisms into pretrained Transformer models. This process includes several steps:\n",
    "\n",
    "1. **Identification of Target Modules**: We identify the key modules within the pretrained model that will benefit from the integration of linearized attention. Using `get_tptt_model`.\n",
    "2. **Modification of Attention Layers**: The identified attention layers are modified to incorporate linearized attention mechanisms. Using `LiZAttention`.\n",
    "3. **Training and Fine-Tuning**: The modified model is then fine-tuned to adapt to the new attention mechanisms, ensuring optimal performance. Using `Peft-LoRA`.\n",
    "\n",
    "## Mathematical Formulations\n",
    "\n",
    "### Softmax Attention\n",
    "Given the input sequence $\\mathbf{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T\\} \\in \\mathbb{R}^{T \\times D}$, the softmax attention mechanism is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} = \\mathbf{X}\\mathbf{W}_Q, \\mathbf{X}\\mathbf{W}_K, \\mathbf{X}\\mathbf{W}_V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{O}_{\\text{base}} = \\text{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{D}}\\right)\\mathbf{V}\n",
    "$$\n",
    "\n",
    "### Linear Attention\n",
    "Linear attention approximates the softmax attention using a feature map $\\phi$:\n",
    "\n",
    "$$\n",
    "\\mathbf{O}_{\\text{lin}, (t)} = \\frac{\\sum_{i=1}^{t} \\phi(\\mathbf{q}_t)^\\top \\phi(\\mathbf{k}_i) \\mathbf{v}_i}{\\sum_{i=1}^{t} \\phi(\\mathbf{q}_t)^\\top \\phi(\\mathbf{k}_i)} = \\frac{S_t}{z_t}\n",
    "$$\n",
    "\n",
    "### Parallel Delta Rules Modeling\n",
    "\n",
    "The delta rule recurrent modeling can be formulated in two forms: closed form and recurrent form. We implement algorithm in Pytorch describes in Parallel Delta Rules paper.\n",
    "\n",
    "##### Closed Form (intra-Chunk)\n",
    "\n",
    "For the closed form, the delta rule is used to update the state within a chunk efficiently. This form allows for parallel computation across the elements of the chunk, making it highly efficient for processing:\n",
    "\n",
    "$$\n",
    "\\mathbf{S}_t = \\mathbf{S}_{t-1} + \\sum_{i=1}^{C} \\mathbf{v}_i \\mathbf{k}_i^\\top\n",
    "$$\n",
    "\n",
    "Here, $C$ is the size of the chunk, and $\\mathbf{S}_t$ represents the updated state for the chunk. This approach leverages matrix operations to compute contributions from each element in the chunk simultaneously.\n",
    "\n",
    "##### Recurrent Form (inter-Chunk)\n",
    "\n",
    "The recurrent form of the delta rule is used to manage updates between chunks. It ensures that the final state of one chunk becomes the initial state of the next, facilitating efficient memory usage for long sequences:\n",
    "\n",
    "$$\n",
    "\\mathbf{S}_{t+1} = \\mathbf{S}_t + \\mathbf{v}_{t+1} \\mathbf{k}_{t+1}^\\top\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{S}_{t+1}$ is the initial state for the next chunk, based on the final state of the previous chunk.\n",
    "\n",
    "### Memory as Gate\n",
    "\n",
    "The Memory as Gate mechanism combines the output of the linear attention $\\mathbf{O}_{\\text{lin}}$ and the base attention $\\mathbf{O}_{\\text{base}}$ using a weighting factor $\\alpha \\in [0,1]$:\n",
    "\n",
    "$$\n",
    "\\mathbf{O} = \\alpha \\cdot \\mathbf{O}_{\\text{lin}} + (1 - \\alpha) \\cdot \\mathbf{O}_{\\text{base}}\n",
    "$$\n",
    "\n",
    "In this context, $\\mathbf{O}_{\\text{lin}}$ is derived from the states $\\mathbf{S}_t$ computed using the delta rule. The states $\\mathbf{S}_t$ capture the necessary information from the sequence, which is then used to compute $\\mathbf{O}_{\\text{lin}}$. This combination allows the model to leverage both the efficiency of linear attention and the robustness of traditional attention mechanisms.\n",
    "\n",
    "\n",
    "## Algorithm Overview\n",
    "\n",
    "### LiZAttention Module\n",
    "\n",
    "The `LiZAttention` module is a core component of the TPTT model, designed to mix linear and vanilla attention mechanisms. It utilizes a cache for storing intermediate states of linear attention layers, supporting a sliding window mechanism to manage memory efficiently.\n",
    "\n",
    "#### Algorithm Steps\n",
    "\n",
    "1. **Projection**:\n",
    "   - Input: `hidden_states` with shape `[batch_size, sequence_length, embedding_dim]`.\n",
    "   - Project `hidden_states` into query (`q`), key (`k`), and value (`v`) matrices.\n",
    "   - Output shapes:\n",
    "     - `q`: `[batch_size, num_heads, sequence_length, head_dim]`\n",
    "     - `k`: `[batch_size, num_key_value_heads, sequence_length, head_dim]`\n",
    "     - `v`: `[batch_size, num_key_value_heads, sequence_length, head_dim]`\n",
    "\n",
    "2. **Attention Mask Application**:\n",
    "   - Apply an attention mask to handle padding and manage sequence length.\n",
    "   - Adjust `v` based on the attention mask to ensure proper alignment with sequence data.\n",
    "\n",
    "3. **Linear Attention Processing**:\n",
    "   - Compute linear attention using the query, key, and value matrices.\n",
    "   - Use a gating mechanism to control information flow.\n",
    "   - Output shape: `o_lin`: `[batch_size, sequence_length, embedding_dim]`\n",
    "\n",
    "4. **Self Attention Processing**:\n",
    "   - Apply self-attention mechanism with truncation to manage long sequences. `o_base`: `[batch_size, max_sequence_length, embedding_dim]`\n",
    "   - Output shape: `o_base`: `[batch_size, sequence_length, embedding_dim]`\n",
    "\n",
    "5. **Combining Outputs**:\n",
    "   - Combine the outputs of linear attention and self-attention using a \"memory as gate\" weighting factor `mag_weight`.\n",
    "   - Final output shape: `[batch_size, sequence_length, embedding_dim]`\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "The training process utilizes Low-Rank Adaptation (LoRA) to fine-tune the model (`peft library`). The LoRA configuration is set with a rank of 8, an alpha of 16, and a dropout rate of 0.05. The training focuses on key projection modules, specifically `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]` for models like Llama/Mistral, or `[\"qkv_proj\", \"out_proj\"]` for models like OpenELM.\n",
    "\n",
    "##### Training Setup\n",
    "\n",
    "Training is conducted with a dataset of 500 samples from the `yahma/alpaca-cleaned` dataset over 5 epochs. Tokens are processed with a maximum length of 384, using truncation and padding as needed. The training is executed on the Kaggle platform, utilizing NVIDIA Tesla T4 GPUs to accelerate computations and handle the workload efficiently.\n",
    "\n",
    "\n",
    "##### Training Parameters\n",
    "\n",
    "Key training parameters include a batch size of 3, a learning rate of 5e-4, and gradient clipping at 1.0 to prevent gradient overflow. Mixed precision training is enabled to optimize computational resources. Training progress is logged every 5 steps, with model outputs saved at the end of each epoch for further analysis. Total step training is 420.\n",
    "\n",
    "##### LiZA MaG Callback\n",
    "\n",
    "The LiZA MaG callback mechanism adjusts the Memory as Gate (MaG) weight during training, facilitating a smooth transition of the model's reliance on linearized attention versus vanilla attention mechanisms. This callback starts with an initial weight of 0.01 and transitions to a final weight of 0.5 over 100 steps. This gradual adjustment helps the model balance between linear and vanilla attention, optimizing performance throughout the training process. The callback is integrated into the training loop to dynamically adjust the MaG weight, enhancing the model's adaptability and efficiency.\n",
    "\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "This part present preliminary result for training and evaluation of model with low compute and tiny model size (around 1B).\n",
    "\n",
    "### Training results\n",
    "\n",
    "The trained models are available at [Hugging Face Model Hub](https://huggingface.co/ffurfaro/). All detailed training metrics are available on the Hugging Face TensorBoard.\n",
    "\n",
    "The results of the training process for various `Titanesque` models are summarized in the table below:\n",
    "\n",
    "   Model | Learning Rate (Final) | Loss (Final) | Training Runtime (sec) | Samples per Second | Steps per Second | Total FLOPs | Gradient Norm (Final) |\n",
    " |-------|-----------------------|-------------|------------------------|-------------------|-------------------|------------|----------------------|\n",
    " | Titans-Llama-3.2-1B | 1.19e-06 | 1.375 | 1654.117 | 1.511 | 0.254 | 5.62e18 | 2.68 |\n",
    " | Titans-OpenELM-1_1B | 1.19e-06 | 1.3188 | 1651.0658 | 1.514 | 0.254 | 5.85e18 | 0.704 |\n",
    " | Titans-Qwen2.5-1.5B | 1.19e-06 | 1.2568 | 1900.6452 | 1.315 | 0.221 | 7.56e18 | 1.985 |\n",
    " | Titans-OLMo-1B-hf | 1.19e-06 | 1.3068 | 1585.151 | 1.577 | 0.265 | 6.20e18 | 3.117 |\n",
    "\n",
    "The training results indicate consistent performance across different models, with variations in training runtime and efficiency metrics. The models demonstrate effective learning as indicated by the final loss values and gradient norms. The use of Low-Rank Adaptation (LoRA) and the Memory as Gate (MaG) mechanism has proven effective in optimizing the training process, as evidenced by the relatively low final loss values and stable gradient norms. These results suggest that the TPTT approach is robust and can be applied across various model architectures to achieve efficient and effective training outcomes.\n",
    "\n",
    "### Experimental Setup\n",
    "We conducted extensive experiments to evaluate the effectiveness of our TPTT method. The experiments were performed on various benchmarks, including language modeling, common-sense reasoning, and long-sequence tasks with lighteval library. The results demonstrate significant improvements in both efficiency and accuracy compared to baseline models.\n",
    "\n",
    "### Benchmark Datasets\n",
    "- **Language Modeling**: We used standard language modeling datasets to evaluate the performance of our model in generating coherent and contextually relevant text.\n",
    "\n",
    "We evaluated the performance of our models using the MMLU benchmark, focusing on the one-shot learning scenario due to RAM limitations. The results are summarized in the following table:\n",
    "\n",
    "   Model | Exact Match (EM) | EM Std Error | Partial Exact Match (PEM) | PEM Std Error | Partial Quasi Exact Match (PQEM) | PQEM Std Error |\n",
    " |-------|-----------------|-------------|----------------------|---------------|----------------------------|----------------|\n",
    " | Titans-Llama-3.2-1B | 0.2456 | 0.1276 | 0.2649 | 0.1340 | 0.4772 | 0.1569 |\n",
    " | Llama-3.2-1B | 0.0070 | 0.0058 | 0.3105 | 0.1411 | 0.4719 | 0.1530 |\n",
    " | Titans-Qwen2.5-1.5B | 0.0000 | 0.0000 | 0.5000 | 0.1504 | 0.5825 | 0.1442 |\n",
    " | Qwen2.5-1.5B | 0.0000 | 0.0000 | 0.5982 | 0.1422 | 0.6895 | 0.1288 |\n",
    " | Titans-OLMo-1B-hf | 0.0000 | 0.0000 | 0.2614 | 0.1312 | 0.4649 | 0.1540 |\n",
    " | OLMo-1B-hf | 0.0000 | 0.0000 | 0.2333 | 0.1302 | 0.4246 | 0.1533 |\n",
    "\n",
    "The results from the MMLU benchmark indicate that the TPTT models generally show improvements in performance compared to their base counterparts. Notably, the Titans-Llama-3.2-1B model demonstrates a significant increase in Exact Match (EM) scores, suggesting better accuracy in generating correct answers. The Partial Exact Match (PEM) and Partial Quasi Exact Match (PQEM) scores also show competitive performance, indicating that the TPTT approach enhances the model's ability to capture relevant information and generate coherent responses. These findings highlight the effectiveness of integrating linearized attention mechanisms and advanced memory management techniques in improving model performance on complex language understanding tasks.\n",
    "\n",
    "## Conclusion\n",
    "In this paper, we presented TPTT, a method-framework to enhance the capabilities of pretrained Transformer models by integrating efficient linearized attention mechanisms. Our approach significantly improves the performance and scalability of large language models, making them more suitable for a wide range of applications. Future work will focus on further optimizing the integration process and exploring additional techniques to enhance model performance.\n",
    "\n",
    "## References\n",
    "- Vaswani, A., et al. (2017). \"Attention Is All You Need.\" Advances in Neural Information Processing Systems.\n",
    "- Brown, T., et al. (2020). \"Language Models are Few-Shot Learners.\" arXiv preprint arXiv:2005.14165.\n",
    "- Katharopoulos, A., et al. (2020). \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention.\" International Conference on Machine Learning.\n",
    "- Chintala, S. (2023). \"Efficient Transformers: A Survey.\"\n",
    "- Team, A. (2023). \"Advances in Neural Language Models.\"\n",
    "- Zhu, X., et al. (2024). \"Recent Trends in Natural Language Processing.\"\n",
    "- Qu, L., et al. (2024). \"Challenges in Scaling Language Models.\"\n",
    "- Kwon, H., et al. (2023). \"Optimizing Key-Value Caches in Transformers.\"\n",
    "- Sun, Y., et al. (2024a). \"Efficient Training of Large Language Models.\"\n",
    "- Yang, Z., et al. (2023). \"Linear Transformers for Efficient Language Modeling.\"\n",
    "- Qin, Y., et al. (2024b). \"Advancements in Linear Recurrent Models.\"\n",
    "- Du, J., et al. (2025). \"Next-Generation Foundational Architectures.\"\n",
    "- MiniMax, et al. (2025). \"Scaling Language Models Efficiently.\"\n",
    "- Mercat, A., et al. (2024). \"SUPRA: Scaling Up Pretrained Models.\"\n",
    "- Wang, L., et al. (2024). \"MambaInLlama: Efficient Linearization Techniques.\"\n",
    "- Zhang, X., et al. (2024a). \"LoLCATs: Low-Cost Attention Transformers.\"\n",
    "- Zhang, X., et al. (2024d). \"Optimizing Gated Linear Recurrent Models.\"\n",
    "- Hu, E. J., et al. (2021). \"LoRA: Low-Rank Adaptation of Large Language Models.\"\n",
    "- Lan, D., et al. (2025). \"Liger: Linearizing Large Language Models to Gated Recurrent Structures.\"\n",
    "- Yang, S., et al. (2024). \"Parallelizing Linear Transformers with the Delta Rule over Sequence Length.\"\n",
    "- Behrouz, A., et al. (2025). \"Titans: Learning to Memorize at Test Time.\"\n",
    "- Furfaro, F. (2025). \"TPTT: Transforming Pretrained Transformer into Titans.\" [GitHub Repository](https://github.com/fabienfrfr/tptt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
